{"pages":[{"title":"about","text":"","link":"/zh-cn/about/index.html"}],"posts":[{"title":"Paper Review - MapReduce (2004)","text":"Review note for MapReduce: simplified data processing on large clusters 1 - SummaryA programming framework, MapReduce, is introduced to easily process large-scale computations on large clusters of commodity PCs. In a MapReduce model, users utilize a self-defined map function on mapping workers to process splits of input data, generating set of intermediate key/value pairs and use a reduce function on reducing workers to sort and merge the intermediate values with the same key. The paper also provides some programming model examples and typical implementations of MapReduce that process terabytes of data on thousands of machines. Some refinements of the model, the performance evaluation of the implementation, the use of MapReduce on indexing system within Google, and related and future work are also discussed in the paper. 2 - ProblemThere are large amounts of data to be processed and generated in many real-world tasks. Although most computations are straightforward, in order to improve efficiency, they need to be distributed across thousands of machines due to the enormous amounts of data. The methods that distribute the data, parallelize computations, and handle failures involve large amounts of complicated code. The problem is how to implement those work in a simple and powerful way and make it fault-tolerant even across numerous machines. 3 - SolutionAuthors designed a framework that dividing the parallelization and distribution tasks into Mapping and Reducing phases in general. Mapping tasks and reducing tasks are assigned to idle workers by the master. In the Mapping phase, input data are partitioned into several splits, and those splits can be processed by different machines (mappers) in parallel, generating intermediate key/value pairs that buffered to local disk. In the Reducing phase, reduce workers remotely read intermediate pairs and sort them by the keys so that those with the same key are aggregated together, and then pass the key and the corresponding list of values to the user-defined Reduce function, which return an output appended to an output file for that reduce partition. All the messy details of partitioning, execution scheduling, inter-machine communications, fault-tolerance, and load balancing are hidden in a library and accommodated by the run-time system. This makes the MapReduce interface simple, manageable, and powerful even for programmers without experience about parallel and distributed system. 4 - NoveltyMapReduce is a brand-new solution of distributed computations framework. It is inspired by primitive functions map and reduce in many programming languages. MapReduce is simple and more fault-tolerant, in comparison to many other contemporary parallel processing systems those are also restricted programming models but have only been implemented on smaller scales and leave machine failures to the programmers to handle. The locality optimization, backup task mechanism, and sorting facility of MapReduce have similarities in spirit to other techniques but are extended from different approaches. 5 - EvaluationAuthors evaluated the performance of MapReduce with two example computations running on a large cluster of PCs. One searches for a specific pattern through about one terabyte of data, the other sorts approximately one terabyte of data. The former shows the progress of the grep program in the aspect of data transfer rate. The latter shows how disabling backup tasks can slow down the execution comparing to the normal execution and demonstrated the strengths of machine failures handling. The differences between MapReduce and other existing methods are mostly discussed in terms of principles. In general, MapReduce is simpler (programmer-friendly), more fault-tolerant, extendable than other parallel processing systems. 6 - OpinionIn the Performance section, the measurements of two implementations (search and sort) only focus on the strengths of MapReduce model and weaknesses are barely discussed. Also, it would be better if there were some comparisons with similar parallelization methods towards the same tasks. One of the most convictive parts to me is the Experience section, stating the progresses and enhancements of MapReduce and its wide-ranging applications within Google, especially the large-scale indexing system. Source:[1] Dean, J. and Ghemawat, S., 2008. MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1), pp.107-113. Key Points: MapReduce is a programming model for processing and generating large datasets. Input -&gt; splitting - -(k/v pairs)- -&gt; Mapping - -(k/v pairs)- -&gt; shuffling -&gt; Reducing -&gt; Output Three states for tasks: idle, in-progress, complete; kept by master along with identity of workers. Everything in-progress on failed workers and completed mapping tasks are reset and will be rescheduled; while completed reducing tasks do not need to, since of output is already stored in a global file system. Pros: simple and powerful interface, hide details of parallelisation and distribution high fault tolerance, the master handles worker failures by re-assigning jobs highly scalable, can be implemented on large cluster Cons: limited for iterative algorithms and interactive data queries due to high disk I/O and replication can be low-efficient, due to stragglers (slow machines) doesn’t support high level lang i.e. SQL Hadoop: MapReduce is a submodule of Hadoop eco-system, a programming model allows to processing large dataset in HDFS; can also run on GFS and other distributed file systems though.","link":"/zh-cn/CS5052-1-MapReduce/"},{"title":"Paper Review - Resilient Distributed Datasets (2012)","text":"Review note for Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing 1 - SummaryThe paper proposed an abstraction for sharing data in cluster application, called Resilient Distributed Dataset (RDD), that is more efficient, general-purpose and fault-tolerant in comparison to existing data storage abstractions for clusters, allowing programmers to process in-memory computations. RDDs are implemented in a big data processing engine, called Spark, and evaluated by a range of user applications and benchmarks in the paper. 2 - ProblemMany emerging applications (e.g. PageRank, K-means clustering, machine learning) and the iterative algorithms behind them, as well as interactive data mining, rely on the reuse of intermediate results across multiple computations. However, most existing cluster computing frameworks adopted for data-intensive analytics are inefficient on this. Their way to reuse data depending on writing results to external stable storage system, can consists of large amount of computing execution time due to data replication, disk I/O, network bandwidth, etc. Some specialized programming models developed for this concern (e.g. Pregel, HaLoop) are not general-purpose. Therefore, RDDs are designed to address computing needs that were met previously by introducing new specialized models. 3 - SolutionRDDs as a new solution to accommodate the problems mentioned above, formally present as a read-only, partitioned collection records. It provides a programming interface in Spark based on coarse-grained transformations, from which lineage dataset is built rather than the actual data. RDDs contain all the information about how it was derived from its lineage or original, actual data in stable storage, hence lost partitions can be quickly recovered. In general, two aspects of RDDs, persistence and partitioning, enable users to reuse intermediate results in memory, optimize data parallelization and efficiently manipulate them by a range of operators (e.g. map, filter). RDDs show good performance on real-world applications those naturally apply same operations to multiple data items. 4 - NoveltyTraditional in-memory data storage abstractions for cluster computing (e.g. distributed shared memory, key-value stores, etc.) depends on fine-grained updates to shared state, involving data replication and logging updates across machines, which can be resource-costly and time-consuming for large-scale computation. RDDs provides a novel solution based on coarse-grained transformations of data items, realizes an efficient, fault-tolerant data storage abstraction, and its special property also guarantees that an RDD that didn’t recover from a failure cannot be referenced by programs.In particular, comparing to other in-memory based systems, such as Piccolo, DSM etc., in which the interface only reads and updates mutable states of dataset, whereas RDDs provide a high-level interface enabling user to manipulate data by many operators. Also, the lineage mechanism of RDDs ensured quicker and cheaper partition recovery. 5 - EvaluationRDDs and Spark the system they are implemented in, are evaluated based on a range of experiments on Amazon EC2, including benchmarks against Hadoop with respect to iterative machine learning and PageRank, fault recovery, behaviour with insufficient memory, some measurements of user applications, and interactive data mining.The strengths of RDDs and Spark reflected on their performance on iterative algorithms (logistic regression and K-means in Machine Learning) and data-intensive analytics, greatly surpassed Hadoop on speeds through storing data in memory hence avoiding replication and I/O traffic, in addition, require less RAM. The fault recovery mechanism based on lineage only reconstructing the lost partition and the extra time cost is subtle. The ability to interactively query large amount data. RDDs and Spark are suitable for a wide range of applications and can express not only specialized programming models for iterative computations, but also new applications not captured by them.One weakness is that RDDs highly rely on the memory space to store job’s data, the performance downgrades in situations of insufficient memory. The limitation is that RDDs excel only at parallel applications that apply the same operations to multiple elements of dataset due to its coarse-grained transformation and immutable property (read-only). 6 - OpinionSpark and its core principle, RDDs, shook up the place of Hadoop and MapReduce in the field of open-source distributed computing frameworks and broke through their limitation on iterative and interactive works. Moreover, it is compatible with HDFS, hence can fit in the ecosystem of Hadoop. The paper gracefully introduced the principle behind RDDs and Spark, demonstrated its advantages against existing frameworks and provided convincible evaluations in many aspects.","link":"/zh-cn/CS5052-2-RDD/"},{"title":"Paper Review - Pregel (2009)","text":"Review note for Pregel: A System for Large-Scale Graph Processing 1 - SummaryLarge graphs have been under analysing for years due to their ubiquity and commercial values, while the existing approaches have many limitations in terms of locality, efficiency, flexibility, etc. Google introduced a vertex-centric computational model framework in this paper that is suitable for large-scale graphs processing on clusters of numerous commodity computers in a manner that developers can easily program with an abstract API without concerning distribution-related details behind it. The paper describes Pregel, the large-scale graph processing model, and associated C++ API, discusses its implementation issues, applications to some algorithms, performances results, and also points out the future directions. 2 - ProblemMore large-scale graphs are being produced and processed for decades. Custom distributed platform demands considerable efforts to implement and not flexible enough to fit new algorithm or graph representation. Frequently used distributed computing infrastructures are often not suitable for graph processing and can lead to suboptimal performance and usability issues. Single-computer graph algorithm libraries compromise the scalability of problems. There are indeed some existing parallel graph process approaches, but they do not address fault tolerance or other critical issues for distributed systems. None of the options ideally fit the comprehensive purposes of large-scale graph processing, being flexible, scalable, fault-tolerant and efficient. 3 - SolutionTo address the problems mentioned above, Google presents a vertex-centric system called Pregel.The computational model takes an initialised graph with unique vertex identifiers as input, and a sequence of iterations (a.k.a. supersteps) will then be carried out, in each of which the user-defined functions are invoked in parallel onto vertices, expressing the given algorithm. The vertex-centric philosophy makes sure that the mechanisms for detecting execution order within each iteration are hidden and communications are simply presented as from one iteration to the next. In one iteration, a vertex can receive messages sent from the previous iteration, propagate messages to other vertices along outgoing edges that will be received at the subsequent iteration, modify the states of its own and outgoing edges and mutate graph topology. The supersteps are organised by global synchronization points, and the synchronicity guarantees that the programs intrinsically avoid deadlocks and data races hence have competitive performances compared to asynchronous systems.Many real-world applications, for example, Page Rank, Shortest Paths, etc., have been deployed and more are being devised under Pregel. 4 - NoveltyThe paper proposed a new solution for large-scale graph processing.Compared to Sawzall, Pig Latin, and Dryad, it hides distribution details and provides a natural API.Compared to MapReduce and other dataflow-based model, the stateful-vertex-focused philosophy make the model more efficient for iterative computation. The idea of synchronous superstep model is actually from Bulk Synchronous Parallel (BSP) models. There are also many other models using BSP implementations, but they are not graph-specific, and their scalability and fault tolerance have been assessed on large clusters.Compared to similar model like Parallel Boost Graph Library, Pregel are more fault tolerant due to explicit messaging mechanism. 5 - EvaluationPregel are implemented on a cluster of 300 commodity machines and evaluated by a range of experiments on SSSP implementation in terms of runtime.The results show that Pregel is efficient at processing very large graphs with billions of vertices and hundreds of billions of edges on huge cluster.The evaluation is not quite telling or comprehensive, because of some flaws stated below Checkpointing was disabled in the experiments, and the fault tolerance had not been verified. Lack of comparisons to other distributed graph processing models, similar or different. Only the application to SSSP was evaluated, lack of evaluations based on many common graph algorithms. 6 - OpinionA cliché about centralized system is that it can be tricky if the master failed, and this has not been discussed in the paper.And as they concluded, there are still some aspects to be improved. For example, the barrier synchronization can be inefficient in cases that some faster workers need to wait for the slower; there is no particular partitioning mechanisms or models coping with a variety of graph topologies in order to balance the loads among workers.Overall, Pregel is a significant and influential framework in the field of graph processing and also a great infrastructure of distributed computing. Key Points:Model characteristics master-slave architechture vertex-focused; stateful (active/inactive) explicit messaging mechanism, through directed edges global synchronization points for each superstep (iteration); highly straggler-sensitive Checkpointing checkpointing before a superstep; kind of like backup master regularly send ping message to workers, if not hearing back, mark failure. re-assign graph partitions to other workers, recovery from the most recent checkpoint. Drawback: too frequent checkpoints might cost more than the expected recovery.","link":"/zh-cn/CS5052-6-Pregel/"},{"title":"权力的游戏 第8季第3集：面对死亡之神我们说什么？","text":"最近大家都是与剧透抗争的战士。前几季，观众：主角们都死了还看个屁？？？最终季，观众：主角们为什么还活着？？？ 万万没想到，铺垫了整整「七季+两集」的大战，这才episode 3呢Arya两秒反杀直接就把Night King干成了一地渣渣，真 • 一地渣渣。开播前各种预测，猜谁死的都有。🙃观众还是naive，也就权游的编剧敢这么写了。 打赌赢了2 out of 3个人头，还是骄傲的。","link":"/zh-cn/GoT-S8-E3/"},{"title":"Paper Review - Mesos (2013)","text":"Review note for Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center 1 - SummaryThis paper introduces Mesos, a platform for sharing commodity clusters across multiple different distributed computing framework. Mesos can be regarded as the kernel of a distributed operating system, it provides fine-grained resource management and runs on every machine of the cluster. 2 - ProblemDistributed systems have been widely implemented as a major computing platform. No perfect computing frameworks will be qualified for all applications, and organisations need to run multiple frameworks for different computing requirements in the same cluster. Because it can improve cluster utilization and allow frameworks to share access to large datasets that is too costly to replicate across clusters. The existing solutions for sharing a cluster are not efficient at data sharing and not highly utilizing the cluster, because of the mismatch of the allocation granularities between the solutions and computing frameworks.For example, frameworks have different scheduling needs, and the scheduling system have to scale to clusters of enormous nodes. And very importantly, the system must be fault-tolerant and highly available. 3 - SolutionMesos is designed to sort out the problems above. Mesos introduces a decentralised scheduling model abstraction called resource offer, delegating the control over scheduling to the frameworks by push resources that frameworks can allocate on a cluster node to run tasks. Namely, Mesos decides how many cluster resources to offer based on a particular policy, while frameworks decide which resources to accept and run tasks on them. Mesos allow fine-grained sharing across diverse computing frameworks by providing a common interface for accessing cluster resources. 4 - NoveltyMesos is a new solution to share a cluster across multiple computing framework. Mesos is fine-grained at the level of tasks and introduces a brand-new distributed scheduling mechanism, resource offer, delegating the control over scheduling to the frameworks by push resources that frameworks can allocate on a cluster node to run tasks. 5 - EvaluationMesos is evaluated through a series of experiments on Amazon EC2. The evaluation starts with a microbenchmark between four workloads. Mesos shows better performance on Facebook Hadoop Mix, Large Hadoop Mix and Spark than statically partitioned cluster, in terms of share of cluster and execution time. However, Torque and MPI performed worse on Mesos. The overhead, decentralized scheduling, scalability and failure recovery are also evaluated. 6 - OpinionMesos is a reliable platform for sharing commodity clusters across multiple different distributed computing framework. It achieves high utilization, respond quickly to workload changes, and cater to diverse frameworks while remaining scalable and robust. But it seems that Mesos have a high barrier to entry. Key Points: Build multiple frameworks in the same cluster, pick the best one for each application (i.g. Pregel for graph processing, Spark for Machine Learning). Share access to large datasets among different frameworks, hence improving the cluster ultilization.","link":"/zh-cn/CS5052-8-Mesos/"},{"title":"权力的游戏 第8季第4集：李云龙你开炮啊","text":"得了，这集人设崩个干净。还不如上一集让夜王一波带走。 首先，Jamie 和 Brienne 这是唱哪出啊。明明是惺惺相惜 + 敬佩 + 小暧昧的美妙关系，非得“升华”到床上，搞成狗血八点档。Jamie 拔迪奥无情不说，走之前还要嘴炮一番，“不好意思瑟曦是个烂人，爱她胜过一切为她做尽烂事的我也一直是个烂人”。看着七季以来战力爆表的女骑士穿着睡袍对着渣男离去背影泣不成声的样子，我真的怀疑自己看的到底是不是权游，这离国产脑残言情剧就只差一句 “我配不上你”了。两张好牌一起打臭，真是棒。 还有，詹德利封爵，遂迫不及待向二丫求婚。也就是第二集开车的善后工作。编剧这里是要让二丫南下君临找瑟曦算账之前给詹德利一个交代？但就像一开始这段床戏完全是为了制造话题性一样，完全unnecessary！二丫和猎狗的故事线外加无面者训练已经足够让刺客萝莉形象很饱满了，在观众里的人气也是数一数二的，非要画蛇添足来这么一段。 这两段感情戏实在是太油腻了。败笔败笔。 三傻、龙妈、色后一个比一个横，谁也不服谁，十有八九最后谁也不能如愿。 三傻抱团狼家抱得有些过了，坚信外来的和尚就念不好经，非得让雪诺称王。问题人家雪诺本来就不想当王啊，当个守夜人总司令管屁股大块地儿都被自己人一人一刀捅成筛子，还让他管七国？讲真，一直觉得囧雪就跟Ned一样是个铁憨憨（好像也不算很铁），政治上根本就是个草包，即没那权谋，更没那野心。三傻为什么非给他揽这瓷器活儿，又凭什么觉得他会是比龙妈更好的统治者。 龙妈这边，前七季持续开挂，第八季持续送。前面一直强调血统说自己是 the rightful queen，现在雪诺身世浮出水面了，血统牌打不下去了又开始说命运。一边说着要free the world from tyrants，一边又说no matter the cost即使代价本身就是tyrants。 不过龙妈的处境也确实让人揪心。第七季为了救雪诺，还没开战就先折一条龙。第八季带着全部战力来临冬城和北境人民一起迎战夜王，然而即使是尸鬼大军压境这样危急存亡的时刻，以三傻为首的北境人也不待见她。从这方面讲，北境真的是有点白眼”狼”了，也不想想，龙妈完全可以像瑟曦一样的置身事外。至少已经白送你们龙晶做武器了（不然真的0胜算），凭什么还要抛弃立场，千里迢迢忍着水土不服跑去北境打前锋，损兵折将，让死敌瑟曦在背后坐收渔利？虽说夜王是二丫收割的，但打前锋的是多斯拉克骑兵、掩护撤退的是无垢者军团、龙焰AOE全场、还把夜王击落到地面，这绝对是又有功劳又有苦劳吧？然而战后狂欢最尽兴的都是北境人，灰虫子和弥桑黛这样的外来者根本无法融入，乔拉和多斯拉克首领死了，龙妈一个人坐在那里，没朋友没家人，忠心耿耿的部下还死了一半，看着雪诺被簇拥被歌颂，心里孤独 + 王位岌岌可危，心态可不是就要崩吗。 龙妈总是被诟病靠男人和开挂上位，中后期也确实开始变得有点狠毒和强硬。但是小恶魔说很对啊，”君主就是要有一定威慑力”。也是权游魅力之所在啊，为什么非要完美人设。 最后说说片尾十分钟对标”亮剑”的城楼戏。 一开始很多人说提利昂后期智商下限我还不以为然，这集真的没法洗了。都什么时候了，还想着用爱感化瑟曦呢？？她都委身攸伦、派刺客杀Jamie、高利贷雇佣黄金团、放她痛恨的君临贱民进红堡了，这明显是孤注一掷不死不休的架势。而且刚刚干死一只龙，还抓了干部弥桑黛，又逢龙妈大军半死半残，北境援军还在赶路。这会儿手持人质站在制高点，数架巨弩随时准备屠龙，简直不要更得意。这种场面下提利昂还怼到城墙根儿来嘴炮劝降，excuse me ??? 还是科本头脑比较清醒。。 这集被传是史上最差一集真是不冤枉。唯一喜欢的戏份就是三傻和猎狗的互动了。 归根结底《权力的游戏》并不是《冰与火之歌》啊。前三集 A Song of Ice and Fire 是为了平衡龙狮战力，后三集争夺铁王座才是真的点题 Game of Throne 啊。看第五集预告里攸伦的一脸震惊外加背景龙吟声，估计Drogon要穿盔甲，不会再给巨弩当活靶子了。 希望编剧最后两集能给圆好了，别烂尾。下一部这种现象级的电视剧不知道要到什么时候了。","link":"/zh-cn/GoT-S8-E4/"},{"title":"正则表达式与grep, sed, awk的基本用法","text":"1. Regular Expression 正则表达式正则表达式常简称为regex。一个regular expression常被成为一种pattern（模式、模板？），用来描述和匹配一系列符合某种规则的字符串。以下是一些基本用法。 1.1 Quantifier 数量限定符注意区别通配符‘*’ Char Description Example ? 表示前面的token出现0或1次 app?le 可匹配 aple(0次)、apple（1次） + 表示前面的token出现1或n次 go+gle 可匹配 gogle（1次）、google、gooogle……（n次) * 表示前面的token出现0或n次 ali(ba)* 可匹配 ali（0次）、aliba（1次）、alibaba……（n次） {n} 表示前面的token固定出现n次 o{2} 可匹配zoo、moon、book……但是Bob不行 {n,} 表示前面的token至少出现n次 {1,}相当于+；{0,}相当于* {m,n} 表示前面的token出现m～n次 As above. Note: 默认为greedy matching，匹配最长的字符串；当?添加在其他任意quantifier后面时，表示lazy matching，匹配最短的字符串。e.g.h.+l matches &#39;hell&#39; in &#39;hello&#39;, greedy.h.+?l matches &#39;hel&#39; in &#39;hello&#39;, lazy. 1.2 其他常用符号 Char Description Comments ^ 字符串starting position ^a 可匹配’alias’，line-based tools中表示每一行的开头 $ 字符串的ending position n$ 可匹配’qq.cn’，line-based tools中表示每一行的结尾 () grouping ali(ba)*中‘ba’是一个tokon . 除了\\n(new-line)之外的任意单个字符 常配合quantifier.+表示1-n个任意字符 \\ 转义符 转右边特殊字符为它本身\\^=^；转右边普通字符为特殊意义\\n=newline；Highest priority | Boolean OR gr(e|a)y 匹配 ‘grey’和’gray’注：由于hexo对markdown的解析还不够完善，此处vertical bar无法成功转义，总是打乱表格，故补充在引用里。 2. grep(Globally search a Regular Expression and Print)2.1 Syntaxgrep [ opt ]... pattern [ file/dir/stdin ]...支持以下三种RegEx引擎-E POSIX扩展正则表达式，ERE (使用除了*之外quantifier时，|时)-G POSIX基本正则表达式，BRE（默认）-P Perl正则表达式，PCRE 2.2 Options &amp; Arguments Options Description -a –text 将binary-file作为文本来进行匹配 -b –byte-offset 在输出行的行首显示offset -c –count 打印每个文件匹配到的行数 -i –ignore-case 忽略大小写 -n –line-number 显示匹配文本所在行的行号 -v –invert-match, 反向匹配，输出不匹配行的内容 -r –recursive 递归匹配 -A n after，除了列出匹配行之外，还列出后面的n行 -B n before，除了列出匹配行之外，还列出前面的n行 –color=auto 将输出中的匹配项设置为自动颜色显示 特殊pattern Description [:digit:] 0-9 [:upper:] A-Z [:lower:] a-z [:alpha:] A-Z, a-z [:alnum:] 0-9, A-Z, a-z [:punct:] punctuation symbol [:graph:] [:alnum:]+[:punct:] [:blank:] [space] and [Tab] [:space:] whitespace: tab, newline, vertical tab, form feed, carriage return, and space. [:cntrl:] 控制按键，包括 CR, LF, Tab, Del..等 [:print:] [:alnum:]+[:punct:]+space [:xdigit:] Hexadecimal：0-9, A-F, a-f Note: 使用这些special pattern时要额外加一对中括号“[]”。 [^]当^在中括号里时，表示排除 2.3 Example1234$ grep '[[:lower:]]' file1 #匹配含有'a-z'的行。 双重中括号。$ grep '[^sci]' file2 #排除s、c、i三个字符，而非‘sci’字符串。$ grep -E '^(w|h)' file3 #匹配以'w'或'h'开头的行。使用'|'需加上'-E'选项。$ grep -E 'w&#123;3&#125;' file3 #匹配含有'www'的行。 使用quantifier需加上'-E'选项。 3. sed(Stream editor)sed是一个非交互式（non-interactive）命令行文本编辑器，用于过滤、转换文本，配合script和正则表达式来同时处理一个或多个文本文件。 3.1 Syntaxsed [ opt ]... {script} [ file/stdin ]... 3.2 Options opt Description -n –silent 安静模式，只打印受影响的行。默认打印stdin的全部行。 -e –expression 添加script -f –file 文件中的命令 -r 使用ERE，默认为标准正则表达式 -i 直接修改输入文件内容，而不是打印到stdout -s –separate 以files为分割，而不是file的每一行。此时stdin为多个文件。 3.3 Script syntax[addr]X[opts] 需用-e选项引导，或作为第一个non-option argument；也可写在file里用-f选项引导。 script内多条命令可用semicolon“;”分隔。 命令a, c, i,后不可跟“;”，故muti-cmd时需放在最后一个。 [addr] 单行 ：n 连续多行 ：n1,n2 从n1至n2。 跳跃多行 ：n~s 从n开始，以s为基数跳跃。如1~2奇数行，2~2偶数行。 Regex ：用正则表达式取址。如/^foo/以“foo”开头的行。 后面加上“!”表示排除 X[opts] p print 打印 通常配合-n开启安静模式 d delete 删除 s substitute 代替（pattern字符串） a append 向后添加 i insert 向前插入 c change 更换（整行） 其中s/regex/text/[flag]，[flag]可以是g（替换行内全部匹配），数字n（替换行内第n个匹配），p替换成功后打印。 其中a, i, c命令可加backslash“\\”换行后再写text。 3.4 Example123456#delete以“foo”开头的行，替换所有行的第3个“hello”为“world”$ sed ’/^foo/d ; s/hello/world/3’ input.txt &gt; output.txt #替换所有不含“apple”的行为“hello there”$ sed ’/apple/!c\\hello there’ input.txt &gt; output.txt 4. awk文本处理语言4.1 工作原理按行(record)读取，按空格（FS）进行切片(field)，将每片保存在内建变量中，$1,$2,$3….。$0表示全部。可以对单个片断进行判断，也可以对所有断进行循环判断。 4.2 Syntaxawk [ -F fs ] [ -v var=value ] [ &#39;script&#39; | -f scriptfile ] [ file ... ] 4.3 Builtin Var var content FS field separator 默认为一个空格 RS record separator 默认为一个\\n NF number of field 默认为片段个数 NR number of record 默认为文件行数 FNR number of record in each file OFS 输出 field separator ORS 输出 record separator ARGC 命令行arguments个数 ARGV 命令行arguments数组 FILENAME 当前输入文件名 4.4 常用awk &#39;{print $2}&#39; 打印第二field。 和 cut -d‘ ’ -f2掐头去尾的用法异曲同工 5. Reference[1]魏镇坪：Linux之awk详解[2]EmanLee: awk 用法（使用入门）","link":"/zh-cn/SHELL_grep_sed_awk/"},{"title":"It's always the little things","text":"This post is to test local audio.It’s always the little things var ap = new APlayer({ element: document.getElementById(\"aplayer-RbrLQCkl\"), narrow: false, autoplay: false, showlrc: false, music: { title: \"It's always the little things\", author: \"郭顶, 山形瑞秋\", url: \"/audio/LittleThings郭顶Yamagata.mp3\", pic: \"/audio/LittleThings郭顶Yamagata.jpeg\", lrc: \"\" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); It’s always the little things That seep into your soul How can I lie here still awake Something inside of me is telling me to go I’ve been waiting to tell you so Mmm Take me home and make it right I long to see the light I’m waiting for a sign And I don’t know if we should part What does it mean if we give up How can I still feel this love for all that we’ve been through And I don’t know what I should say Is it the time to walk away Should we let go or should we stay And find a way through Mmm Take me home and make it right Can you still see the best of me Or am I falling out of sight It’s always the hardest thing To listen to your soul When something is telling you to go A part of you wants to fight And part just fly away How I long to tell you so And I don’t know if we should part What does it mean if we give up How can I still feel this love For all that we’ve been through And is it time to walk away Should we let go or should we stay How do you know when something’s too far gone to get back to Mmm Take me home and make it right Can you still see the best in me Or am I falling Am I falling Am I falling out of sight","link":"/zh-cn/test-music/"},{"title":"“男与女”","text":"var ap = new APlayer({ element: document.getElementById(\"aplayer-ujRAswkD\"), narrow: false, autoplay: true, showlrc: false, music: { title: \"14. 정사 [ 남과 여 OST]\", author: \".\", url: \"amaaw.mp3\", pic: \"/zh-cn/amaaw/amaaw.jpeg\", lrc: \"\" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); 🌙 晚安","link":"/zh-cn/amaaw/"},{"title":"平凡料理：皮很脆的照烧鸡腿肉","text":"测试iframe视频插入。就放vv的平凡料理吧。 youtube tag 1&#123;% youtube 4IOI-98_d9c %&#125; 皮很脆的照烧鸡腿肉 on YouTube。 iframe 1&lt;iframe width=\"360\" src=\"https://www.youtube.com/embed/4IOI-98_d9c\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt; 第一次就大成功大满足的蒜香牛排饭 on Bilibili。 看饿了。","link":"/zh-cn/test-video/"},{"title":"CS5052 Data-Intensive Systems - Revision Note","text":"Revision Checklist Week 1 - 2 slides Week 3 - 11 papers Previous Exams Examinable Topics Core Papers Parallel Processing 1. MapReduce2. RDD (Resilient Distributed Datasets)3. GFS (Google File System) Storage and Databases 4. CAP and BASE theorem5. Amazon’s Dynamo Graph and Stream Processing 6. Pregel Resource Managers 7. Google’s Borg8. Mesos Cloud Engineering 9. Performance Variation and Predictability in Public IaaS Clouds 10. HotSpot Edge / Decentralised 11. Incremental deployment and migration of geo-distributed situation awareness applications in the fog Machine Learning and Systems 12. Learning Scheduling Algorithms for Data Processing Clusters Distributed Systemsa collection of independent computers that appears to the user as a single coherent system. Tanenbaum, 2006 Complex:components are autonomous and collaborative. Design issues: Heterogeneity. Network/hardware/OS/languages are heterogenous; use standard protocols (HTTP) Openness, taken for granted at networking level but not universal at component level; determines whether the system can be extended or re-implemented in various ways; new resource sharing service can be added by clients; key interfaces published; uniform communication mechanisms; Security, might be mutually incompatible security policies, more ways of being attacked comparing to centralised systems; Scalability, size (add more resource), distribution (geo-disperse without degrading), manageability. Scaling up (upgrade resources), scaling out (add new). Techniques: hiding latency (avoid blocking on remote), distribution, replication Failure handling; usually partial, detection difficult and can cascade; must provide high degree of availability. Techniques: detecting/masking failures (retransmitting), design tolerating client, recovery policy, redundancy (database replication) Transparency, ideally user not aware distribution, practically impossible (parts are independently managed and network latency); usually make users aware so that can cope with problems (node crash); Types: hide access/location/migration/replication/concurrency/failure/scaling Quality of Service, reflects the system’s ability to deliver services reliably with a response time/throughput acceptable to users. (e.g. if quality falls then sound/video so degraded that impossible to understand) Centralised (2-tier) and Decentralised (N-tier/P2P) distributed computing systems Homogeneous Cluster computing: largly the same, physically close, highly connected, ONE admin domain Heterogeneous Grid computing: no assumptions about hardware/OS/network, MULTIPLE domains Cloud computing: elastic capacity, pay-per-use, on-demand self service, resources are virtualised Structure of Distributed SystemsThe structure of distributed systems top to bottom: Applications/Services Middleware Operating System Computer and Network Hardware Application/Service (Layer 1) has client-server architecture. A server for a distributed database may act as a client, forwarding requests to different file servers. Three levels: UI, processing, data. In a 2-tier client-server (centralised) architecture, singple logical server mounting indefinite number of clients. Thin-client model, only presentation layer implemented on client, all others on server. simple to deploy and manage the clients heavy processing load on both server and network Fat-client model, some/all of the application processing is locally executed on client. suitable when the capabilities of client system are known in advance more complex than thin, expecially for management of new clients less network traffic new versions have to be installed on all clients In a Multi-tier client-server (decentralised: vertical distribution) architecture, seperate processes execute on different physical machines/servers. Alleviate problems with scalability, potentially offer higher performance. In a peer-to-peer (decentralised: hotizontal distribution) architecture, seperate shares of a complete set of data are operated on several logically-equivalent clients/servers. Attr: processes on peers are all equal; interactions are symmetric; each process acts as both client and server PRO: Resilience; Scalability CON: no-one’s in charge; each peer only knows limited info Middleware (Layer 2) is the software that provides useful building blocks, manages the diverse components (involving diff lang/processors/protocols) and ensure they can communicate and exchange data. OS and hardware (Layer 3 &amp; 4) are platform. Some trivial points: Virtual Machine: VMWare, XenSource Vittual Storage: GFS, HDFS Big data: high-volume, high-velocity, high-variaty; must be converted to knowledge and understanding in order to stimulate scientific discoveries, industry innovations and economic growth. Cloud Computing To developers: no upfront infrastructure investment reduced operating costs through utility pricing elastic on-demand architecture high service availability ease of use To providers: exploit existing data-centre capacity take advantage of economies of scale (available to purchasers of extremely large volumes of hardware and network capacity) Enabled technologies: Virtualization: abstract physical infrastructure, isolated guest OS co-existing on the same computer (see figure below) Virtual storage: GFS, HDFS Service-Oriented Architecture (SOA): REpresentational State Transfer(REST); develop applications using existing services, make applications available as services or both Tight Coupling: components difficult to understand in isolation, have many interdependency, difficult to maintain and re-use, cascade changes Loose Coupling: components independent; easier maintainance and re-use Ultility Computing: pay-per-use, on-demand Different types of cloud: IaaS (infrastructure), Amazon Web Services deliver raw computer infrastructure vendors manage networking, hard drives, OS etc. PaaS (platform), Microsoft Azure deliver an application framework hosted framework user can built applications on SaaS (software), Google Mail deliver applications as a service Terminology Machine Images: a template that contains software configuration Instances: different instance types are essentially different hardware archetype Regions: geo areas of the world which instances can be deployed in, each region contains multiple Availability Zones (AZ), low latency network within the same region Elastic Compute Cloud (EC2): a service providing resizable computing capacity in Amazon’s data centres Simple Storage Service (S3): Internet storage, REST and SOAP interfaces for app dev Glacier: low-cost storage service, optimized for infrequently accessed data, e.g. retrievsl times of several hours (achiving research/scientific data etc.) Metrics: CPU utilization, latency, request counts. Some custom app/sys metrics: memory usage, transaction volume, error rate Cloud vendors commonly provide Software Development Kit (SDK) for different languages, hide low-levels from end-users with abstract interface. SummaryCloud Computing provides resizable computing capacity that enables users to build and host applications in a data centre. computing as a utility elastic capacity pay-per-use self service interface resources virtualized","link":"/zh-cn/CS5052-revision/"},{"title":"MT5761 Statistics Modeling - Revision Note","text":"Questions and solutions in previous exam paper. Numbers in square brackets indicate marks. 1. lm, glsCorr, glsCorrVar [14](a) Describe predict power of a lm model. Comment model performance over some covariates e.g. time! [2][1] Reasonably poor; fairly low $R^2$ 0.2; poor agreement between observed and fitted [1] Prior to A are underestimated, between A to B are overestimated, post B are underestimated. (b) Three assumptions of lm model; Descibe validity of them. [3][1] Normality assumption, appears to be violated, Shapiro-Wilk $H_0$ NORMAL [1] Constant error variance assumption, no evidence violated, Bresh-Pagan test $H_0$ CONSTANT [1] Independence assumption, clearly violated, correlation shown in $\\text{acf}$ plot &amp; Durbin-Watson test $H_0$ INDEPENDENT, test stats less than 2 (c) Think’bout source of data, other reason of correlation. [1]Realistic reason. (d) Describe mean-var relationships underlying BP test, contrast with that assumed by glsVar. [2][1] Breusch-Pagan Test use $r_i^2 = \\alpha_0 + \\alpha_1x_i + \\gamma_i$ to determine the extent of agreement between residual variance and vars $x$, where $r_i^2$ are squared residuals, $\\alpha_0$ and $\\alpha_1$ are estimated in the model. [1] BP test assume the test statistics $NR^2\\sim \\chi^2_p$ in this case degree of freedom p = 1. GLS model assume $r^2_i \\sim N(0,\\sigma^2|\\hat{y_i}|^{2m})$ or $N(0,\\sigma^2e^{2my_i})$ (e) What do BP test and glsVar suggest about the mean-var relationship? [3][1] BP test checked(not constant): No evidence for a linear relationship with squared residuals. [1] glsVar: There appears to be a non-zero power-based relationship. [1] AIC smaller when the power coefficient is fitted, and zero is not a plausible in the gls model. (f) Conclude which model is most defensiable. [3][1] Overall Conclusion: There is no/strong evidence for a change of RespVar over ExpVars. [1] Best fitting model (based on AIC [1], it’s glsCorrVar) exhibits a large/small p-value (0.05) for the relationship. The models which inappropriately ignore something concludes ExpVar is (not) significant. 2. glm for Poisson(OD) [11](a) Relationship impied by glmPois model on RAW and LINK scale. Which is suitable. [3][1] glmPois assumes a nonlinear relationship between RespVar and ExpVars and a linear relationship on log/sqrt scale. [1] Not good for this data; [1] model allow monotonic relationship but function seems to need inflection points. (b) Mean-Var relationships underpins glmPois and glmPoisOD. Which is more realistic. [3][1] glmPois assume fitted mean and residual var are equal. [1] glmPoisOD assume residual var is proportional to fitted mean. $\\text{var} = \\phi \\lambda$ [1] In this case, latter is more realistic since estimate of dispersion parameter $\\hat\\phi=399$ is much larger than 1. (c) Contrast conclusions of glmPois ,glmPoisOD andglsCorrVar, which is most defensiable. Additional methods to improve. [3][1] glmPois and glmPoisOD suggest strong evidence for negative/positive Resp-Exp relationship. ​ glsCorrVar suggest this not well-evidenced, could be sampling variability alone. [1] would use glsCorrVar to base my conclusion on. Because although looks nonlinear (even Poisson-based are barely linear on raw scale), the non-constant error variance and correlation are modelled in errors. [1] smoother based function(splines) to improve the relationship? GEE(Generalized Estimating Equation) much like GLMs but allow non-independence. (d) How to investigate the effects of XXX or XXX on RespVar? If RespVar available at both. [2][2] e.g. Interactions, piecewise linear models, time as a factor variable. 3. glm for Binomial [15](a) Describe fit.logit , including RespVar, linear predictior, random component, link function. [4][1] Linear predictor [1] Link function [1] Intercept and Error term [1] Other β parameters Assuming $y_i \\sim Binomial(n_i,p)$ , where $y_i$ is the no. of observation with XXX out of the $n_i$ observations, and $p$ is the probability of XXX (being caught with fish in stomach), also the RespVar of the model, varying over ExpVars.$$p_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} + \\epsilon_i\\$$Linear predictor is obtained by transforming RespVar by the Link function, which is$$g(p_i)=\\log{(\\frac{p_i}{1-p_i})}=\\eta_i=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4x_4+\\beta_5x_5$$$\\beta_0$is the intercept parameter (baseline), represents $\\text{female, &lt;2.3m}$ and $\\text{george}$. $\\beta_1$is the coefficient for male (compared with female) $\\beta_2$is the coefficient for &gt; 2.3m (compared with &lt; 2.3m) $\\beta_3$is the coefficient for hancock (compared with george) $\\beta_4$is the coefficient for oklawaha (compared with george) $\\beta_5$is the coefficient for trsfford (compared with george) $\\epsilon_i$ is the random component - the binomial error term (b) Odds of Binomial GLM [2]$$\\text{Odds} = \\frac{p(\\text{success})}{p(\\text{failue})} = \\frac{p_{it}}{1-p_{it}}= e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4x_4+\\beta_5x_5}$$ (c) Calculate odds use fit.logit.best [2]$$\\text{odds(small,trafford)} = e^{\\beta_0+\\beta_4}=e^{-0.1218-1.3733} = 0.2242262$$ (d) Multiplicative effect between odds for different levels [1][1] For baseline level 0 compared with non-base level n, the odds of presence(success) vs absence(failure) are estimated to change by a factor $e^{\\beta_n}$ (e) Explain Deviance [4][1] Deviance provides a measure of discrepancy between fitted model and Saturated model. The smaller the D, the better the model.$$D = 2[l(\\hat\\beta_{sat},\\phi) - l(\\hat\\beta,\\phi)]$$[1] If model correct, $D \\sim \\chi^2_{n-p-1}$, n observations, p predictors ​ A $\\chi^2$ test ($H_0$ model is correct) will give a large p-value for a good fitting. [1] $\\chi^2$ approximation of D often poor for Binomial GLMs [1] Computing $D$ involves $\\phi$, so if it’s unknown we can’t use the result. (f) adjustment to raw residuals and why [2][1] Adjustment.$$ \\text{Pearson residuals} = \\frac{\\text{raw residual}}{\\hat{\\text{SD}}}=\\frac{y-\\hat{y}}{\\sqrt{\\text{Var}(y)}}$$ [1] Why. We’d like to see no patterns in Pearson residuals if Mean-Var relationship is appropriate. 4. glm for Multinomial [10](a) Nominal and Ordinal [1]Nominal: Response value categories has no natural order. e.g. Ordinal: The matter of response matters. e.g. (b) Assumptions for multinomial GLM fit.mult [3][1] Independent observations from Multinomial distribution. [1] Linear relationship with covars, on (cumulative) log odds scale. [1] IIA : Independence from Irrelevant Alternatives. Assuming the odds of one outcome vs another does not dependent on what alternative outcomes are available. (c) Model Selection procedure to choose covariates[2][1] Fit models with all possible combinations of covariates; dredge [1] use fit criteria (e.g. AIC) to rank models. (d) Calculate response. Reptile; small; hancock[2]$$p_{ij} = \\frac{e^{\\eta_{ij}}}{1+\\sum_{k=2}^J e^{\\eta_{ik}}}$$ $\\eta_3 = -3.66588368 + 1.2431622 =-2.42272148$ $\\eta_2 = -0.09083394 -1.6583241= -1.74915804$ $\\eta_4 = -2.72380722 + 0.6952142 = -2.02859302$ $\\eta_4 =-1.57283851+ 0.8262891= -0.74654941$$$p_{ij} = \\frac{e^{-2.42272148}}{1+e^{-1.74915804}+e^{-2.42272148}+e^{-2.02859302}+e^{-0.74654941}}=0.04747016$$ (e) Assumption of a proportional odds model [2][1] Proportional odds Assumption (especially for ordinal!): ​ The slope of covar relationship is the same for for each outcome level. [1] To test the validity: ​ Fit a model that does not make it, use a model selection statistics to test between two. General Reasons for using GLM instead of LR: Response Var is not guaranteed to change linearly with Explainatory Vars; Response Var is naturally bounded by some range and LR predictions can produce values outside this range; Errors are unlikely to be normal with constant variance.","link":"/zh-cn/MT5761-revision/"},{"title":"MT4113 Statistic Computing - Revision Syllabus","text":"Revision syllabus for MT4113 by lectures/chapters. Fundamental programming principlesComputer (lecture 1) [x] Definition of a computer [x] Hardware, software [x] Programming languages: classification, generations, [x] compiled vs interpreted, which should be used when Some Definition Computer - A device that accept info and manipulates it according to a sequence of instructions to produce output Hardware - a physical computer equipment, including CPU, memory (Register, RAM), mass storage(ROM), I/O (keyboard,screen,printer) Software - computer program, a set of instructions operating the hardware. System sw(Linux, Win), App sw(SAS, Word), Programming sw(R) Programming LanguagesClassification by level Low - close to hardware, machine lang. High - far from hardware, close to natural lang. run on diff OS Generality - high lvl. more specialized in application Generation - 1st ~ 5th Speed/Efficiency: Low level generally faster(not always) Generations 1st: Machin lang. - Relatively few diﬀerent instructions 2nd: Assembly lang. - Human-readable machine lang. 3rd: CPU-independent lang. - Fortran, C, Pascal, OOLs(C++, Java). most we use written in 3GL 4th: designed with a speciﬁc application; database query lang(SQL) graphical user interface(GUI) maths lang(Mathematica, Maple) stats lang(R, SAS) 5th: to solving problems given problem speciﬁcation; no need explicitly writting algorithm Prolog. Used is in artiﬁcial intelligence studies. Compiled vs InterpretedTwo approaches to turn langs into machine langs(must be done b4 canbe executed): Interpret: do it line by line when entering using an ‘interpreter’, run it straight way. Python, R Compile: written and save in file, turn all into machine code in one go using a ‘compiler’. Which and When Simple do once stuff - GUI, no reproducible trail more complex few times - 4GL within interpreter prodiction where efficiency important - 3GL and compile, or prototype in a 4GL plus slow bits in 3GL Algorithms (lecture 2) [x] Definition; important features; components of an algorithm [x] Examples, criticism of examples [x] Code and pseudocode [x] Control structures and vectorization in R What’s An AlgorithmAlgorithm: an ordered sequence of unambiguous and well-deﬁned instructions, performing some task and halting in ﬁnite time! Important features an ordered sequence unambiguous, well-defined(clear, do-able, canbe done without difficulty) perform tasks, nothing left out halt in finite time, so algorithm needs to be terminate Example boil a egg! Elements assigning values computation return values Control Structures Conditional exec - if, switch case Iteration (looping) - repeat a set of instr fixed number variable number (warning! potential infinite loop) Recursion - algorithms that call themselves Code and PseudocodeCode: instructions in computer lang that implements an algorithm. Pseudocode: instructions that are generic, informal, lang-unspecified, specify an algorithm. Always write out pseudocode b4 coding! VectorizationMuch faster than loops Some variants of apply - sapply, lapply, tapply, mapply, %by% Modularization and good programming practice (lect 2, 3, prac 2, 3) [x] Modular programming, definition, advantages, interface, passing by reference and value, best practices, encapsulation [x] Environments, scoping, dynamic lookup [x] Strategies for code design [x] Coding conventions and style [x] Testing and debugging. Types of code errors; how to build tests; debugging strategies. [x] Online collaborative programming tools: git repositories, clone, pull, push, commit. ModularizationModular programming！split program into discrete, readable blocks of code so that each block does a small amount. Function Advantages avoid repeating easier to test esaier to update and propagate changes divide and conquer problems Module interfaces passing by value make copies, stored in separate location from original var changes inside function have no aﬀect on var’s value outside inefficient but safer passing by reference directly using original var (location) changes to the variable within function aﬀect its’ value after the function run efficient but dangerous Most 3GLs allow both interfaces; one can specify interfaces for certain parameters in C; Base R does not allow passing by reference. Best practice/concept of Encapsulation pass everything required as an argument pass everything out using return() No global side effects within functions Environment (Workspace?!) Base environment (contains basic packages) Global environment (pkg loaded, var created), “reset” every time reopen R Current environment (environment inside of a function) Scoping Name masking: looks up one level for undefined names(can rewrite predefined var, e.g. pi) R default to the version in most recently loaded pkg/func for same name func use of function independent of any previous (env wiped clean for new use) What’s Dynamic Lookup It only matters when code is executed, rather than created. Design Strategies Top-down (rigid) and bottom-up (iterative) approaches flowcharts for planning and documentation outlines or pseudocode (breaking big task into manageable bits) Coding ConventionsBasically readablity and consistency &lt;– for assignment, = for argument, == for boolean function comments listing purpose, I/O indentation and spacing around operators and after commas lines &lt;80 characters long meaningful names in consistent style Testing and DebuggingType of errors: syntax errors logic errors numerical errors (overflow/underflow/floating point problems) How to build test: what code should produce how does code behave with special case (0, nagetive, NA, gigantic) build test prior to building code (code is created to pass tests) Debugging strategy: Trial and error global arguments and step through func print() object as function progresses Computer arithmetic (lecture 4) [x] How numbers are stored on computers. Fixed point – addition, multiplication, overflow. Floating point – limitations, consequences of lack of accuracy. [x] Data size on computer. [x] Fixed and floating point numbers in R. [x] Living with floating point limitations – strategies to diagnose and avoid problems. [x] Character storage. Fixed Point (32bits integer in R) addition 就直接加 multiplication 就是 shift and add overflow, 加起来超位数了！ binary 乘以 2的n次幂 = 小数点右移补零 binary 除以 2的n次幂 = 小数点左移舍位 $n\\times 12 = n \\times (2^3) + n \\times (2^2)$ n添3个零 + n添2个零 Pro - results are exact, fast Con - limited applicability and range In R, 32 bit integer 最大是 $2^{31}-1$ Float Point (numeric in R)Represented as $S.F \\times B^E$ $S$ - Sign $F$ - Fraction, unsigned integer $B$ - Base, 2 for computer $E$ - Exponent, signed integer, shift left/right e.g. $-.110\\times 2^{1011} = -.000110 = -0.9375$ Limitations bits for $F$ (fraction) limits the accuracy bits for $E$ (exponent) limits the size $\\epsilon = 2^{1-f}$, the smallest positive number (interval) can be distinguished consequences of lack of accuracy rarely add up exactly, due to rounding err order of summation matters (can’t overflow at the beginning) subtraction of two nearly equal numbers eliminates the most significant digits Pro - wide applicability, wider range Con - result not exact, slow In R defualt numeric 64 bits in total, 1 for sign, 11 for exponent, 53 for fraction (1 extra bit by a trick) largest $+.111…_{53} \\times 2^{+1023} = 8.98 \\times 10^{+307}$ accuracy $\\epsilon = 2^{1-53} = 2.20\\times 10^{-16}$ Underows are set to 0, no warning Live with limit of Float Point re-work expression avoid overflow Normalize, scale by dividing change exact test to ‘good enough’ test, from x == y to x - y &lt; e careful about order of operation double-precision(desperate) Many show a trade-off between accuracy and speed Character Storage as binary, determined by character set ASCII, 7-bit, 128 unique chars Unicode, 16-bit, 65536 chars Data Size bit: binary digit, smallest unit of storage on a machine, hold 0/1 byte: 8 bits - hold a single character word: natural data size of a computer, most 64 bits kilobyte (KB) $2^{10}$ = 1024 bytes megabyte (MB) $2^{20}$ = 1024 KB gitabyte (GB) $2^{30}$ = 1024 MB Computer-intensive statistics (lect 6, 7, 8)An understanding of how these methods work, ability to reproduce the algorithms and apply to case studies, pros and cons – which to use when. [x] Overview of parametric and nonparametric methods for constructing tests and confidence intervals. Contexts include one sample, two sample, ANOVA, multiple regression. [x] Permutation methods – test and interval [x] Randomization methods – test and interval [x] Monte Carlo tests; Monte Carlo methods [x] Bootstrap – nonparametric (including balanced bootstrap) and parametric. Percentile confidence intervals. Traditional Par and Non-par MethodTraditional Parametric method, specify PDF for population is known, $f(\\theta) \\sim N(\\mu, \\sigma^2)$; distribution of test statistic $T(x)$; Problems: result variant to transformation; bias correction. Traditional Non-parametric method, avoid fully specifying $f()$; e.g. Wilcoxon Signed Rank test: $f()$ is symmetric about some median m; $T(X)$ is specified Cons: Wilcoxon e.g. require to formulate $H_0$ in terms of median; Less powerful than a t-test when parametric assumptions met Computer-intensive Statistical MethodAvoid fully specifying either $f()$ and/or $T()$, e.g. Permutation-based approach Randomization-based approach Monte-Carlo-based version of t test (avoid $T()$, but not $f(\\theta)$) Par and Non-Par Bootstrap CI (avoid $T()$, $f(\\theta)$ depends) Permutation MethodsAssume $x_i$ is symmetric about $\\mu_0$. Given 12 obs., there are $2^{12} = 4096$ ways (permutaions). $p$-value is the proportion of that $|\\bar{X}|$ greater then observed $|\\bar{x}|$, how extreme the observed mean is. e.g. 48 out of 4096 are as extreme - p = 48/4096 = 0.012 Permutation tests pick $H-0$ and $T(X)$ under $H-0$ all permutation are equally likely calculate the $p$-value (proportion as extreme) free to choose whatever test statistic e.g. median, trimmed mean, t-stat, etc. readily extends to other simuation e.g. two-sample, ANOVA, multi regression Permutation CI Find the set of $\\mu_0$ such that $H_0: \\mu = \\mu_0$ is not rejected Pros - widely applicable; produce exact result; no specific distribution assumption for population; no analytic distribution of test statistic. Cons - each permutation must be equally likely (or prob is known); hard to program; can be prohibitively intensive for large sample size. Randomization MethodsRamdomization is like permutation, except that only random subset of all permutations. e.g. randomly assign $|x_i-\\mu_0|$ to each Compared with Permutation Methods Pros - all pros of it plus; easier to code; number of randomization is fixed rather than $2^{n}$ Cons - each permutation must be equally likely (or prob is known); results vary. Monte-Carlo TestsMonte-Carlo tests is a generalization of randomization test to include parametric distribution. Algorithm as Set population distribution $f()$ or $f(\\theta)$ and $H_0$ Simulate a dataset under $H_0$ and calculate $T(X)$ (repeat 999 times) Add $T(x)$ of sample data quantile method - calc $p$-value (proportion of $T(X)$ as extreme or more than the one of sample data $T(x)$) Pros - Can be more flexible than randomization, par or non-par; no analytic distribution of test statistic; can cope with non-iid data. Cons(compare to permutation) - need to extimate parameters for par; results vary. BootstrapBootstrapping simulations can be Parametric, simulate from a distribution that properties based on sample data (no iid assumption) Non-parametric, simulate by resampling with replacement from sample data (from Empirical DF) Percentile Method, 95% CI given by 2.5% and 97.5% percentile of the $\\mu$ set. Pros Simple, general and robust, Cons poor performance when underlying distribution is very skewed or estimator is very biased generally, only asymptotically exact as $B$ and $n \\to \\infty$ $f()$ assumption and iid assumption are respectively pro and con for Par and Non-par Bootstrapping. General Monte Carlo MethodMonte Carlo Methods are a broad class of computational algorithms that rely on repeated random sampling to generate numerical results. Used for Inference, (computer-intensive stats, e.g. MC test, Bootstrap, Bayesian stats) Optimization, (later) Stochastic Numerical Integration (compared to Deterministic one!) Optimization methods (lect 9, 10, 11)An understanding of how the methods work, ability to reproduce the algorithms and suggest which might work best in case studies, pros and cons, which to use when. An understanding of what the R functions nls, optimize and optim do. [x] Components of optimization [x] Outline of approaches: graphical, analytic, brute force, iterative: “basic”, calculus-based, stochastic [x] Systematic: Line and grid searches [x] For iterative approaches: convergence, convergence order, stopping rules, robustification [x] Bisection; golden section [x] Newton’s method – univariate, multivariate [x] Gauss-Newton [x] The EM algorithm. [x] Stochastic optimization: optimizing stochastic functions; stochastic search methods. Blind random search, localized random search, simulated annealing. [x] Not examinable: tabu methods and genetic algorithms. Why optimization? non-linear Least-squares problems Maximum Likelihood find CI via test inversion Minimizing risk in Bayesian decision analysis Components of Optimization Objective function: what we want to maximize/minimize, e.g. logLik for ML, $(y-\\hat{y})^2$ for GLS Criteria: Constraints on parameters Stopping Rule(s): desired accuracy/maximum iterations Approaches 优化途径 Graphical - esay; but can’t quantify error, hard for multivariate problems Analytic - aka differentiate func then set differential to zero; solution exact; but there’s often no analytic solution! Brute force - line/grid search (for univariate/multivariate problems); reliable, with known error; inefficient: require numerous calculation dependes on accuracy ($\\epsilon$), hard for multivariate space; ‘Brute force’ could be used to find starting value for other methods to refine Iterative methods “basic”: plotting, line or grid searches Calculus-based (lect 11), Newton, Gauss-Newton Stochastic (lect 12) General Iterative Algorithm Start iteration $t=0$, initial guess $x^0$ Next iteration $t=t+1$, improved guess $x^t = f(x^{t-1})$, using an updating equation Evaluate whether new guess is sufficiently accurate, using a stopping rule If yes, stop and return $x^ = x^t$ and optimized objective function $g(x^)$ If no, go back to 2 or consider whether to give up (report no convergence) Bisection SearchTake finding root of $g’(x) = 0$ as an example. Start with initial interval $[a_0, b_0]$, and $g’(a_0)g’(b_0)&lt;0$, and systematically shrink the interval half by once. Golden Section Search More efficient (converge more quickly), as an extension of bisection method that takes andvantage of the Golden Ratio. ConvergenceEach iteration produces an answer closer to the optimum, we gradually converge on the solution we want. Convergence order is an index of how fast it converges. Convergence criterion serves as stopping rules, two main types, absolute and relative Absolute: stop when $|{x^t - x^{t-1}|} &lt; \\epsilon$, max tolerance. Problems: regardless of the size of the numbers, e.g. err of 0.1 is usually more important when $x^\\ast$ is 0.5 than it is 50,000 Relative: $\\frac{|x^t - x^{t-1}|}{|x^{t-1}|} &lt; \\epsilon$, max proportional tolerance. Robustification Use more numerically stable operation e.g. use $a+\\frac{b-a}{2}$ instead of $\\frac{a+b}{2}$ “good enough” convergence (helps when $x^t$ close to zero) $$\\frac{|x^t-x^{t-1}|}{|x^{t-1}|+\\epsilon} &lt; \\epsilon$$ stop anyway after $N$ iteration, whether convergence achieved or not stop if no converging over several iterations, restart somewhere else Global optimum may not be bracketed at the beginning! or even if bracket it, there may be multiple optima, you may not turn out getting the best one. Newton’s Method (univariate)Also called “Newton-Raphson Method” Iterative, numerical technique for optimization, based on linear approximation with Taylor Series. Think of it as using Successive tangent lines! RationaleApproximate $g’(x^\\ast)$ by linear Taylor series expansion at $x^t$$$g’(x^\\ast) \\approx g’(x^t) + (x^\\ast-x^t)g’’(x^t) = 0$$ Re-arranging gives$$x^\\ast \\approx x^t - \\frac{g’(x^t)}{g’’(x^t)}$$ Here requires that $g’(x)$ is continuously differentiable, and $g’’(\\ast)\\ne 0$ So, the updaing equation is$$x^{t-1} = x^t - \\frac{g’(x^t)}{g’’(x^t)}$$ Algorithm define objective function $g(x)$ and convergence criterion tolerance $\\epsilon$ start iteration $t=0$, set $x^0$, calculate $g(x^0)$. next iteration $t=t+1$, update $x^{t+1} = x^t - g’(x^t)/g’’(x^t)$, calculate $g(x^{t+1})$. evaluate whether convergence criteria are met. if met, return $x^\\ast = x^t$ and exit; otherwise, return to step 3. Summary of Newton’s Can converge etremely rapidly - convergence order $\\beta=2$ but, may not converge at all, depending on shape of $g’(x)$ Can easily get stuck in local optima Newton’s Method (multivariate) Objective function is $g(\\theta)$ where $\\theta$ is a vector of $n$ parameters $\\nabla$ and $\\nabla^2$ is the first and second derivative (gradient and Hessian) $\\Delta$ is the vector of each direction (for one step), $\\theta_{1} = \\theta_0 + \\Delta$ $$\\nabla^2 g(\\theta) \\times \\Delta = - \\nabla g(\\theta)$$ where $\\nabla^2g(\\theta)$ is an $n \\times n$ matrix; $\\Delta$ is a $1 \\times n$ vactor; ​ $\\nabla g(\\theta)$ is an $n$ by $n\\times 1$ vector. Algorithm Pseudocode set objective function $g(\\theta)$ and convergence tolerance $\\epsilon$ chose starting $\\theta_0$ calculate $g(\\theta_0)$ $g’(\\theta) = \\nabla$ $g’’(\\theta) = \\nabla^2$ $\\nabla^2 g(\\theta)\\Delta = - \\nabla g(\\theta)$ , solve for $\\Delta$ $\\theta_{1} = \\theta_0 + \\Delta$ Gauss-Newton Method (multivariate)Iterative procedure for extimating parameters of nonlinear least-square problems(GLS). Objective function is always the sum of square of residuals. Only first order derivatives required (advantage over Newton’s) Rationale Linearise the regression with a 1st order Taylor series evaluated at $\\theta^0$ re-arrange in the form of linear regression $$\\begin{aligned}f(x,\\theta) - f(x,\\theta^0) &amp;= \\sum_j (\\theta_j - \\theta^0_j)\\frac{df} {d\\theta_j}\\bigg|_{\\theta^0} \\&amp;\\equiv \\sum_j \\gamma_j\\omega_j\\end{aligned}$$ where $\\gamma_j = \\theta_j-\\theta_j^0$ corresponds to the unknown parameter $\\omega_j = \\frac{df}{d\\theta_j}\\Big|_{\\theta^0}$ corresponds to the known covariate Algorithm start with $i=0$, set initial parameter $\\theta^0$ estimate $\\gamma$:$$\\gamma^1 = (W^{0T}W^0)^{-1} \\bullet W^{0T}z^0$$ update $\\theta^1 = \\theta^0 + \\gamma^1$ re-evaluate $W$ and $z$ at $\\theta^1$ if met, return $\\theta$; if not go to step 2. Summary of Newton and Gauss-NewtonNewton’s Method Any objective functions require 1st and 2nd derivatives Uni and Multivariate case Gauss-Newton Method Objective function always sum of square of resid require only 1st derivatives for Multivariate non-linear least-square problem Stochastic MethodsPrevious methods are all deterministic methods. OBJ function can be evaluated exactly; algorithm is deterministic - i.e. given same data and start point, it takes same path to same solution. Why Stochastic Necessary to deal with stochastic func Can be used for discrete problem do better in finding global rather than local! some Cons; faster than brute force but slower than deterministic. Want a good solution or the best? Stochastic optimization methods use random variables. Two variaties below can overlap: Optimizing stochastic functions (OBJ function contains random noise); step size decrease, accuracy increase as iteration number increase Optimization algorithm with random search (search methods contains random element); converge faster, improve robustness when global optimum hard to find. Stochastic search Algorithm Blind random search - generate new candidate from some chosen PDF Localized random search - generate new candidate from some chosen PDF (depends on current candidate); step neither fixed(brute force) nor full random (blind) Simulated annealing - many variants, work well but sloooow Consequences of Stochasticity Evaluation of OBJ func (and its gradient) no longer accurate grid search methods may pick wrong point; iterative methods may fail to converge Accuracy can be improved by repeat eval, err decrease with $1/\\sqrt{N}$ Reproducibility and graphics (lect 12, 13, 14)Reproducible research [x] What is it and why is it a good idea [x] Ways to achieve reproducibility (or fail to achieve reproducibility) [x] Key components of reproducible research [x] Not examinable: specifics of R Markdown. Replicable, independent researchers can follow the same methods and arrive at the same conclusions. OLD. Reproducible, at least the result should be reproducible: using same data and code yield the same result Why Reproducible can reproduce the project even forget most details after long time easier to run with addtional data planning for longevity makes research more impactful Using different tools for each phase of project lead to bad reproducibility, because err can occur in any/all of these phases new data arise, whole process start over Components of Reproducibility Publication with linked and executable code and data Never touch raw data avoid mannual data manipulation version vontrol include random seeds info use relative paths Also, file project components well! Graphical presentation of data [x] What makes a good graphic; Tufte’s principles; graphical integrity [x] Ability to recognize good and bad graphics and explain why they are good or bad [x] Not examinable: specifics of data visualization methods in R. Tufte’s Principles numbers match true proportions dimensions match the data labels clear and detailed use well-known units to represent money not vary for some ulterior motive and not imply unintended context Lie factor - is the ratio of effect shown in graphic to effect shown in data.$$\\text{Lie Factor} = \\frac{\\text{size of effect shown in graphic}}{\\text{size of effect shown in data}}$$Chart Junk - unnecessary complexicy in graphic Code performance [x] What a profiler is and how and when to use one [x] Ability to recognize what might benefit from optimization given profiler output Profiler - software that records the instructions computer is executing When to use - after code is working as intended. Tidyverse [x] Definition of tidy data [x] Ability to recognize tidy vs. untidy data [x] Not examinable: specific R tidyverse packages and functions. Tidy data is easier to manipulate, model and visualize! define as Each variable is saved in its onw column Each observation is saved in its own row","link":"/zh-cn/MT4113-revision/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","link":"/zh-cn/tags/MapReduce/"},{"name":"university","slug":"university","link":"/zh-cn/tags/university/"},{"name":"exam","slug":"exam","link":"/zh-cn/tags/exam/"},{"name":"Hadoop","slug":"Hadoop","link":"/zh-cn/tags/Hadoop/"},{"name":"Spark","slug":"Spark","link":"/zh-cn/tags/Spark/"},{"name":"Pregel","slug":"Pregel","link":"/zh-cn/tags/Pregel/"},{"name":"Mesos","slug":"Mesos","link":"/zh-cn/tags/Mesos/"},{"name":"Linux","slug":"Linux","link":"/zh-cn/tags/Linux/"},{"name":"Hexo","slug":"Hexo","link":"/zh-cn/tags/Hexo/"},{"name":"vlog","slug":"vlog","link":"/zh-cn/tags/vlog/"},{"name":"P2P","slug":"P2P","link":"/zh-cn/tags/P2P/"},{"name":"分布式","slug":"分布式","link":"/zh-cn/tags/分布式/"},{"name":"论文","slug":"论文","link":"/zh-cn/tags/论文/"},{"name":"谷歌","slug":"谷歌","link":"/zh-cn/tags/谷歌/"},{"name":"大数据","slug":"大数据","link":"/zh-cn/tags/大数据/"},{"name":"日常","slug":"日常","link":"/zh-cn/tags/日常/"},{"name":"剧","slug":"剧","link":"/zh-cn/tags/剧/"},{"name":"权力的游戏","slug":"权力的游戏","link":"/zh-cn/tags/权力的游戏/"},{"name":"命令行","slug":"命令行","link":"/zh-cn/tags/命令行/"},{"name":"正则","slug":"正则","link":"/zh-cn/tags/正则/"},{"name":"教程","slug":"教程","link":"/zh-cn/tags/教程/"},{"name":"音频","slug":"音频","link":"/zh-cn/tags/音频/"},{"name":"影","slug":"影","link":"/zh-cn/tags/影/"},{"name":"食物","slug":"食物","link":"/zh-cn/tags/食物/"},{"name":"菜谱","slug":"菜谱","link":"/zh-cn/tags/菜谱/"},{"name":"视频","slug":"视频","link":"/zh-cn/tags/视频/"},{"name":"云计算","slug":"云计算","link":"/zh-cn/tags/云计算/"},{"name":"统计","slug":"统计","link":"/zh-cn/tags/统计/"},{"name":"模型","slug":"模型","link":"/zh-cn/tags/模型/"},{"name":"算法","slug":"算法","link":"/zh-cn/tags/算法/"}],"categories":[{"name":"Note","slug":"Note","link":"/zh-cn/categories/Note/"},{"name":"Log","slug":"Log","link":"/zh-cn/categories/Log/"}]}