{"pages":[{"title":"about","text":"","link":"/zh-cn/about/index.html"}],"posts":[{"title":"Paper Review - MapReduce (2004)","text":"Review note for MapReduce: simplified data processing on large clusters 1 - SummaryA programming framework, MapReduce, is introduced to easily process large-scale computations on large clusters of commodity PCs. In a MapReduce model, users utilize a self-defined map function on mapping workers to process splits of input data, generating set of intermediate key/value pairs and use a reduce function on reducing workers to sort and merge the intermediate values with the same key. The paper also provides some programming model examples and typical implementations of MapReduce that process terabytes of data on thousands of machines. Some refinements of the model, the performance evaluation of the implementation, the use of MapReduce on indexing system within Google, and related and future work are also discussed in the paper. 2 - ProblemThere are large amounts of data to be processed and generated in many real-world tasks. Although most computations are straightforward, in order to improve efficiency, they need to be distributed across thousands of machines due to the enormous amounts of data. The methods that distribute the data, parallelize computations, and handle failures involve large amounts of complicated code. The problem is how to implement those work in a simple and powerful way and make it fault-tolerant even across numerous machines. 3 - SolutionAuthors designed a framework that dividing the parallelization and distribution tasks into Mapping and Reducing phases in general. Mapping tasks and reducing tasks are assigned to idle workers by the master. In the Mapping phase, input data are partitioned into several splits, and those splits can be processed by different machines (mappers) in parallel, generating intermediate key/value pairs that buffered to local disk. In the Reducing phase, reduce workers remotely read intermediate pairs and sort them by the keys so that those with the same key are aggregated together, and then pass the key and the corresponding list of values to the user-defined Reduce function, which return an output appended to an output file for that reduce partition. All the messy details of partitioning, execution scheduling, inter-machine communications, fault-tolerance, and load balancing are hidden in a library and accommodated by the run-time system. This makes the MapReduce interface simple, manageable, and powerful even for programmers without experience about parallel and distributed system. 4 - NoveltyMapReduce is a brand-new solution of distributed computations framework. It is inspired by primitive functions map and reduce in many programming languages. MapReduce is simple and more fault-tolerant, in comparison to many other contemporary parallel processing systems those are also restricted programming models but have only been implemented on smaller scales and leave machine failures to the programmers to handle. The locality optimization, backup task mechanism, and sorting facility of MapReduce have similarities in spirit to other techniques but are extended from different approaches. 5 - EvaluationAuthors evaluated the performance of MapReduce with two example computations running on a large cluster of PCs. One searches for a specific pattern through about one terabyte of data, the other sorts approximately one terabyte of data. The former shows the progress of the grep program in the aspect of data transfer rate. The latter shows how disabling backup tasks can slow down the execution comparing to the normal execution and demonstrated the strengths of machine failures handling. The differences between MapReduce and other existing methods are mostly discussed in terms of principles. In general, MapReduce is simpler (programmer-friendly), more fault-tolerant, extendable than other parallel processing systems. 6 - OpinionIn the Performance section, the measurements of two implementations (search and sort) only focus on the strengths of MapReduce model and weaknesses are barely discussed. Also, it would be better if there were some comparisons with similar parallelization methods towards the same tasks. One of the most convictive parts to me is the Experience section, stating the progresses and enhancements of MapReduce and its wide-ranging applications within Google, especially the large-scale indexing system. Source:[1] Dean, J. and Ghemawat, S., 2008. MapReduce: simplified data processing on large clusters. Communications of the ACM, 51(1), pp.107-113. Key Points: MapReduce is a programming model for processing and generating large datasets. Input -&gt; splitting - -(k/v pairs)- -&gt; Mapping - -(k/v pairs)- -&gt; shuffling -&gt; Reducing -&gt; Output Three states for tasks: idle, in-progress, complete; kept by master along with identity of workers. Everything in-progress on failed workers and completed mapping tasks are reset and will be rescheduled; while completed reducing tasks do not need to, since of output is already stored in a global file system. Pros: simple and powerful interface, hide details of parallelisation and distribution high fault tolerance, the master handles worker failures by re-assigning jobs highly scalable, can be implemented on large cluster Cons: limited for iterative algorithms and interactive data queries due to high disk I/O and replication can be low-efficient, due to stragglers (slow machines) doesnâ€™t support high level lang i.e. SQL Hadoop: MapReduce is a submodule of Hadoop eco-system, a programming model allows to processing large dataset in HDFS; can also run on GFS and other distributed file systems though.","link":"/zh-cn/CS5052-1-MapReduce/"},{"title":"Paper Review - Resilient Distributed Datasets (2012)","text":"Review note for Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing 1 - SummaryThe paper proposed an abstraction for sharing data in cluster application, called Resilient Distributed Dataset (RDD), that is more efficient, general-purpose and fault-tolerant in comparison to existing data storage abstractions for clusters, allowing programmers to process in-memory computations. RDDs are implemented in a big data processing engine, called Spark, and evaluated by a range of user applications and benchmarks in the paper. 2 - ProblemMany emerging applications (e.g. PageRank, K-means clustering, machine learning) and the iterative algorithms behind them, as well as interactive data mining, rely on the reuse of intermediate results across multiple computations. However, most existing cluster computing frameworks adopted for data-intensive analytics are inefficient on this. Their way to reuse data depending on writing results to external stable storage system, can consists of large amount of computing execution time due to data replication, disk I/O, network bandwidth, etc. Some specialized programming models developed for this concern (e.g. Pregel, HaLoop) are not general-purpose. Therefore, RDDs are designed to address computing needs that were met previously by introducing new specialized models. 3 - SolutionRDDs as a new solution to accommodate the problems mentioned above, formally present as a read-only, partitioned collection records. It provides a programming interface in Spark based on coarse-grained transformations, from which lineage dataset is built rather than the actual data. RDDs contain all the information about how it was derived from its lineage or original, actual data in stable storage, hence lost partitions can be quickly recovered. In general, two aspects of RDDs, persistence and partitioning, enable users to reuse intermediate results in memory, optimize data parallelization and efficiently manipulate them by a range of operators (e.g. map, filter). RDDs show good performance on real-world applications those naturally apply same operations to multiple data items. 4 - NoveltyTraditional in-memory data storage abstractions for cluster computing (e.g. distributed shared memory, key-value stores, etc.) depends on fine-grained updates to shared state, involving data replication and logging updates across machines, which can be resource-costly and time-consuming for large-scale computation. RDDs provides a novel solution based on coarse-grained transformations of data items, realizes an efficient, fault-tolerant data storage abstraction, and its special property also guarantees that an RDD that didnâ€™t recover from a failure cannot be referenced by programs.In particular, comparing to other in-memory based systems, such as Piccolo, DSM etc., in which the interface only reads and updates mutable states of dataset, whereas RDDs provide a high-level interface enabling user to manipulate data by many operators. Also, the lineage mechanism of RDDs ensured quicker and cheaper partition recovery. 5 - EvaluationRDDs and Spark the system they are implemented in, are evaluated based on a range of experiments on Amazon EC2, including benchmarks against Hadoop with respect to iterative machine learning and PageRank, fault recovery, behaviour with insufficient memory, some measurements of user applications, and interactive data mining.The strengths of RDDs and Spark reflected on their performance on iterative algorithms (logistic regression and K-means in Machine Learning) and data-intensive analytics, greatly surpassed Hadoop on speeds through storing data in memory hence avoiding replication and I/O traffic, in addition, require less RAM. The fault recovery mechanism based on lineage only reconstructing the lost partition and the extra time cost is subtle. The ability to interactively query large amount data. RDDs and Spark are suitable for a wide range of applications and can express not only specialized programming models for iterative computations, but also new applications not captured by them.One weakness is that RDDs highly rely on the memory space to store jobâ€™s data, the performance downgrades in situations of insufficient memory. The limitation is that RDDs excel only at parallel applications that apply the same operations to multiple elements of dataset due to its coarse-grained transformation and immutable property (read-only). 6 - OpinionSpark and its core principle, RDDs, shook up the place of Hadoop and MapReduce in the field of open-source distributed computing frameworks and broke through their limitation on iterative and interactive works. Moreover, it is compatible with HDFS, hence can fit in the ecosystem of Hadoop. The paper gracefully introduced the principle behind RDDs and Spark, demonstrated its advantages against existing frameworks and provided convincible evaluations in many aspects.","link":"/zh-cn/CS5052-2-RDD/"},{"title":"Paper Review - Pregel (2009)","text":"Review note for Pregel: A System for Large-Scale Graph Processing 1 - SummaryLarge graphs have been under analysing for years due to their ubiquity and commercial values, while the existing approaches have many limitations in terms of locality, efficiency, flexibility, etc. Google introduced a vertex-centric computational model framework in this paper that is suitable for large-scale graphs processing on clusters of numerous commodity computers in a manner that developers can easily program with an abstract API without concerning distribution-related details behind it. The paper describes Pregel, the large-scale graph processing model, and associated C++ API, discusses its implementation issues, applications to some algorithms, performances results, and also points out the future directions. 2 - ProblemMore large-scale graphs are being produced and processed for decades. Custom distributed platform demands considerable efforts to implement and not flexible enough to fit new algorithm or graph representation. Frequently used distributed computing infrastructures are often not suitable for graph processing and can lead to suboptimal performance and usability issues. Single-computer graph algorithm libraries compromise the scalability of problems. There are indeed some existing parallel graph process approaches, but they do not address fault tolerance or other critical issues for distributed systems. None of the options ideally fit the comprehensive purposes of large-scale graph processing, being flexible, scalable, fault-tolerant and efficient. 3 - SolutionTo address the problems mentioned above, Google presents a vertex-centric system called Pregel.The computational model takes an initialised graph with unique vertex identifiers as input, and a sequence of iterations (a.k.a. supersteps) will then be carried out, in each of which the user-defined functions are invoked in parallel onto vertices, expressing the given algorithm. The vertex-centric philosophy makes sure that the mechanisms for detecting execution order within each iteration are hidden and communications are simply presented as from one iteration to the next. In one iteration, a vertex can receive messages sent from the previous iteration, propagate messages to other vertices along outgoing edges that will be received at the subsequent iteration, modify the states of its own and outgoing edges and mutate graph topology. The supersteps are organised by global synchronization points, and the synchronicity guarantees that the programs intrinsically avoid deadlocks and data races hence have competitive performances compared to asynchronous systems.Many real-world applications, for example, Page Rank, Shortest Paths, etc., have been deployed and more are being devised under Pregel. 4 - NoveltyThe paper proposed a new solution for large-scale graph processing.Compared to Sawzall, Pig Latin, and Dryad, it hides distribution details and provides a natural API.Compared to MapReduce and other dataflow-based model, the stateful-vertex-focused philosophy make the model more efficient for iterative computation. The idea of synchronous superstep model is actually from Bulk Synchronous Parallel (BSP) models. There are also many other models using BSP implementations, but they are not graph-specific, and their scalability and fault tolerance have been assessed on large clusters.Compared to similar model like Parallel Boost Graph Library, Pregel are more fault tolerant due to explicit messaging mechanism. 5 - EvaluationPregel are implemented on a cluster of 300 commodity machines and evaluated by a range of experiments on SSSP implementation in terms of runtime.The results show that Pregel is efficient at processing very large graphs with billions of vertices and hundreds of billions of edges on huge cluster.The evaluation is not quite telling or comprehensive, because of some flaws stated below Checkpointing was disabled in the experiments, and the fault tolerance had not been verified. Lack of comparisons to other distributed graph processing models, similar or different. Only the application to SSSP was evaluated, lack of evaluations based on many common graph algorithms. 6 - OpinionA clichÃ© about centralized system is that it can be tricky if the master failed, and this has not been discussed in the paper.And as they concluded, there are still some aspects to be improved. For example, the barrier synchronization can be inefficient in cases that some faster workers need to wait for the slower; there is no particular partitioning mechanisms or models coping with a variety of graph topologies in order to balance the loads among workers.Overall, Pregel is a significant and influential framework in the field of graph processing and also a great infrastructure of distributed computing. Key Points:Model characteristics master-slave architechture vertex-focused; stateful (active/inactive) explicit messaging mechanism, through directed edges global synchronization points for each superstep (iteration); highly straggler-sensitive Checkpointing checkpointing before a superstep; kind of like backup master regularly send ping message to workers, if not hearing back, mark failure. re-assign graph partitions to other workers, recovery from the most recent checkpoint. Drawback: too frequent checkpoints might cost more than the expected recovery.","link":"/zh-cn/CS5052-6-Pregel/"},{"title":"æƒåŠ›çš„æ¸¸æˆ ç¬¬8å­£ç¬¬3é›†ï¼šé¢å¯¹æ­»äº¡ä¹‹ç¥æˆ‘ä»¬è¯´ä»€ä¹ˆï¼Ÿ","text":"æœ€è¿‘å¤§å®¶éƒ½æ˜¯ä¸å‰§é€æŠ—äº‰çš„æˆ˜å£«ã€‚å‰å‡ å­£ï¼Œè§‚ä¼—ï¼šä¸»è§’ä»¬éƒ½æ­»äº†è¿˜çœ‹ä¸ªå±ï¼Ÿï¼Ÿï¼Ÿæœ€ç»ˆå­£ï¼Œè§‚ä¼—ï¼šä¸»è§’ä»¬ä¸ºä»€ä¹ˆè¿˜æ´»ç€ï¼Ÿï¼Ÿï¼Ÿ ä¸‡ä¸‡æ²¡æƒ³åˆ°ï¼Œé“ºå«äº†æ•´æ•´ã€Œä¸ƒå­£+ä¸¤é›†ã€çš„å¤§æˆ˜ï¼Œè¿™æ‰episode 3å‘¢Aryaä¸¤ç§’åæ€ç›´æ¥å°±æŠŠNight Kingå¹²æˆäº†ä¸€åœ°æ¸£æ¸£ï¼ŒçœŸ â€¢ ä¸€åœ°æ¸£æ¸£ã€‚å¼€æ’­å‰å„ç§é¢„æµ‹ï¼ŒçŒœè°æ­»çš„éƒ½æœ‰ã€‚ğŸ™ƒè§‚ä¼—è¿˜æ˜¯naiveï¼Œä¹Ÿå°±æƒæ¸¸çš„ç¼–å‰§æ•¢è¿™ä¹ˆå†™äº†ã€‚ æ‰“èµŒèµ¢äº†2 out of 3ä¸ªäººå¤´ï¼Œè¿˜æ˜¯éª„å‚²çš„ã€‚","link":"/zh-cn/GoT-S8-E3/"},{"title":"Paper Review - Mesos (2013)","text":"Review note for Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center 1 - SummaryThis paper introduces Mesos, a platform for sharing commodity clusters across multiple different distributed computing framework. Mesos can be regarded as the kernel of a distributed operating system, it provides fine-grained resource management and runs on every machine of the cluster. 2 - ProblemDistributed systems have been widely implemented as a major computing platform. No perfect computing frameworks will be qualified for all applications, and organisations need to run multiple frameworks for different computing requirements in the same cluster. Because it can improve cluster utilization and allow frameworks to share access to large datasets that is too costly to replicate across clusters. The existing solutions for sharing a cluster are not efficient at data sharing and not highly utilizing the cluster, because of the mismatch of the allocation granularities between the solutions and computing frameworks.For example, frameworks have different scheduling needs, and the scheduling system have to scale to clusters of enormous nodes. And very importantly, the system must be fault-tolerant and highly available. 3 - SolutionMesos is designed to sort out the problems above. Mesos introduces a decentralised scheduling model abstraction called resource offer, delegating the control over scheduling to the frameworks by push resources that frameworks can allocate on a cluster node to run tasks. Namely, Mesos decides how many cluster resources to offer based on a particular policy, while frameworks decide which resources to accept and run tasks on them. Mesos allow fine-grained sharing across diverse computing frameworks by providing a common interface for accessing cluster resources. 4 - NoveltyMesos is a new solution to share a cluster across multiple computing framework. Mesos is fine-grained at the level of tasks and introduces a brand-new distributed scheduling mechanism, resource offer, delegating the control over scheduling to the frameworks by push resources that frameworks can allocate on a cluster node to run tasks. 5 - EvaluationMesos is evaluated through a series of experiments on Amazon EC2. The evaluation starts with a microbenchmark between four workloads. Mesos shows better performance on Facebook Hadoop Mix, Large Hadoop Mix and Spark than statically partitioned cluster, in terms of share of cluster and execution time. However, Torque and MPI performed worse on Mesos. The overhead, decentralized scheduling, scalability and failure recovery are also evaluated. 6 - OpinionMesos is a reliable platform for sharing commodity clusters across multiple different distributed computing framework. It achieves high utilization, respond quickly to workload changes, and cater to diverse frameworks while remaining scalable and robust. But it seems that Mesos have a high barrier to entry. Key Points: Build multiple frameworks in the same cluster, pick the best one for each application (i.g. Pregel for graph processing, Spark for Machine Learning). Share access to large datasets among different frameworks, hence improving the cluster ultilization.","link":"/zh-cn/CS5052-8-Mesos/"},{"title":"æƒåŠ›çš„æ¸¸æˆ ç¬¬8å­£ç¬¬4é›†ï¼šæäº‘é¾™ä½ å¼€ç‚®å•Š","text":"å¾—äº†ï¼Œè¿™é›†äººè®¾å´©ä¸ªå¹²å‡€ã€‚è¿˜ä¸å¦‚ä¸Šä¸€é›†è®©å¤œç‹ä¸€æ³¢å¸¦èµ°ã€‚ é¦–å…ˆï¼ŒJamie å’Œ Brienne è¿™æ˜¯å”±å“ªå‡ºå•Šã€‚æ˜æ˜æ˜¯æƒºæƒºç›¸æƒœ + æ•¬ä½© + å°æš§æ˜§çš„ç¾å¦™å…³ç³»ï¼Œéå¾—â€œå‡åâ€åˆ°åºŠä¸Šï¼Œææˆç‹—è¡€å…«ç‚¹æ¡£ã€‚Jamie æ‹”è¿ªå¥¥æ— æƒ…ä¸è¯´ï¼Œèµ°ä¹‹å‰è¿˜è¦å˜´ç‚®ä¸€ç•ªï¼Œâ€œä¸å¥½æ„æ€ç‘Ÿæ›¦æ˜¯ä¸ªçƒ‚äººï¼Œçˆ±å¥¹èƒœè¿‡ä¸€åˆ‡ä¸ºå¥¹åšå°½çƒ‚äº‹çš„æˆ‘ä¹Ÿä¸€ç›´æ˜¯ä¸ªçƒ‚äººâ€ã€‚çœ‹ç€ä¸ƒå­£ä»¥æ¥æˆ˜åŠ›çˆ†è¡¨çš„å¥³éª‘å£«ç©¿ç€ç¡è¢å¯¹ç€æ¸£ç”·ç¦»å»èƒŒå½±æ³£ä¸æˆå£°çš„æ ·å­ï¼Œæˆ‘çœŸçš„æ€€ç–‘è‡ªå·±çœ‹çš„åˆ°åº•æ˜¯ä¸æ˜¯æƒæ¸¸ï¼Œè¿™ç¦»å›½äº§è„‘æ®‹è¨€æƒ…å‰§å°±åªå·®ä¸€å¥ â€œæˆ‘é…ä¸ä¸Šä½ â€äº†ã€‚ä¸¤å¼ å¥½ç‰Œä¸€èµ·æ‰“è‡­ï¼ŒçœŸæ˜¯æ£’ã€‚ è¿˜æœ‰ï¼Œè©¹å¾·åˆ©å°çˆµï¼Œé‚è¿«ä¸åŠå¾…å‘äºŒä¸«æ±‚å©šã€‚ä¹Ÿå°±æ˜¯ç¬¬äºŒé›†å¼€è½¦çš„å–„åå·¥ä½œã€‚ç¼–å‰§è¿™é‡Œæ˜¯è¦è®©äºŒä¸«å—ä¸‹å›ä¸´æ‰¾ç‘Ÿæ›¦ç®—è´¦ä¹‹å‰ç»™è©¹å¾·åˆ©ä¸€ä¸ªäº¤ä»£ï¼Ÿä½†å°±åƒä¸€å¼€å§‹è¿™æ®µåºŠæˆå®Œå…¨æ˜¯ä¸ºäº†åˆ¶é€ è¯é¢˜æ€§ä¸€æ ·ï¼Œå®Œå…¨unnecessaryï¼äºŒä¸«å’ŒçŒç‹—çš„æ•…äº‹çº¿å¤–åŠ æ— é¢è€…è®­ç»ƒå·²ç»è¶³å¤Ÿè®©åˆºå®¢èè‰å½¢è±¡å¾ˆé¥±æ»¡äº†ï¼Œåœ¨è§‚ä¼—é‡Œçš„äººæ°”ä¹Ÿæ˜¯æ•°ä¸€æ•°äºŒçš„ï¼Œéè¦ç”»è›‡æ·»è¶³æ¥è¿™ä¹ˆä¸€æ®µã€‚ è¿™ä¸¤æ®µæ„Ÿæƒ…æˆå®åœ¨æ˜¯å¤ªæ²¹è…»äº†ã€‚è´¥ç¬”è´¥ç¬”ã€‚ ä¸‰å‚»ã€é¾™å¦ˆã€è‰²åä¸€ä¸ªæ¯”ä¸€ä¸ªæ¨ªï¼Œè°ä¹Ÿä¸æœè°ï¼Œåæœ‰å…«ä¹æœ€åè°ä¹Ÿä¸èƒ½å¦‚æ„¿ã€‚ ä¸‰å‚»æŠ±å›¢ç‹¼å®¶æŠ±å¾—æœ‰äº›è¿‡äº†ï¼Œåšä¿¡å¤–æ¥çš„å’Œå°šå°±å¿µä¸å¥½ç»ï¼Œéå¾—è®©é›ªè¯ºç§°ç‹ã€‚é—®é¢˜äººå®¶é›ªè¯ºæœ¬æ¥å°±ä¸æƒ³å½“ç‹å•Šï¼Œå½“ä¸ªå®ˆå¤œäººæ€»å¸ä»¤ç®¡å±è‚¡å¤§å—åœ°å„¿éƒ½è¢«è‡ªå·±äººä¸€äººä¸€åˆ€æ…æˆç­›å­ï¼Œè¿˜è®©ä»–ç®¡ä¸ƒå›½ï¼Ÿè®²çœŸï¼Œä¸€ç›´è§‰å¾—å›§é›ªå°±è·ŸNedä¸€æ ·æ˜¯ä¸ªé“æ†¨æ†¨ï¼ˆå¥½åƒä¹Ÿä¸ç®—å¾ˆé“ï¼‰ï¼Œæ”¿æ²»ä¸Šæ ¹æœ¬å°±æ˜¯ä¸ªè‰åŒ…ï¼Œå³æ²¡é‚£æƒè°‹ï¼Œæ›´æ²¡é‚£é‡å¿ƒã€‚ä¸‰å‚»ä¸ºä»€ä¹ˆéç»™ä»–æ½è¿™ç“·å™¨æ´»å„¿ï¼Œåˆå‡­ä»€ä¹ˆè§‰å¾—ä»–ä¼šæ˜¯æ¯”é¾™å¦ˆæ›´å¥½çš„ç»Ÿæ²»è€…ã€‚ é¾™å¦ˆè¿™è¾¹ï¼Œå‰ä¸ƒå­£æŒç»­å¼€æŒ‚ï¼Œç¬¬å…«å­£æŒç»­é€ã€‚å‰é¢ä¸€ç›´å¼ºè°ƒè¡€ç»Ÿè¯´è‡ªå·±æ˜¯ the rightful queenï¼Œç°åœ¨é›ªè¯ºèº«ä¸–æµ®å‡ºæ°´é¢äº†ï¼Œè¡€ç»Ÿç‰Œæ‰“ä¸ä¸‹å»äº†åˆå¼€å§‹è¯´å‘½è¿ã€‚ä¸€è¾¹è¯´ç€è¦free the world from tyrantsï¼Œä¸€è¾¹åˆè¯´no matter the costå³ä½¿ä»£ä»·æœ¬èº«å°±æ˜¯tyrantsã€‚ ä¸è¿‡é¾™å¦ˆçš„å¤„å¢ƒä¹Ÿç¡®å®è®©äººæªå¿ƒã€‚ç¬¬ä¸ƒå­£ä¸ºäº†æ•‘é›ªè¯ºï¼Œè¿˜æ²¡å¼€æˆ˜å°±å…ˆæŠ˜ä¸€æ¡é¾™ã€‚ç¬¬å…«å­£å¸¦ç€å…¨éƒ¨æˆ˜åŠ›æ¥ä¸´å†¬åŸå’ŒåŒ—å¢ƒäººæ°‘ä¸€èµ·è¿æˆ˜å¤œç‹ï¼Œç„¶è€Œå³ä½¿æ˜¯å°¸é¬¼å¤§å†›å‹å¢ƒè¿™æ ·å±æ€¥å­˜äº¡çš„æ—¶åˆ»ï¼Œä»¥ä¸‰å‚»ä¸ºé¦–çš„åŒ—å¢ƒäººä¹Ÿä¸å¾…è§å¥¹ã€‚ä»è¿™æ–¹é¢è®²ï¼ŒåŒ—å¢ƒçœŸçš„æ˜¯æœ‰ç‚¹ç™½çœ¼â€ç‹¼â€äº†ï¼Œä¹Ÿä¸æƒ³æƒ³ï¼Œé¾™å¦ˆå®Œå…¨å¯ä»¥åƒç‘Ÿæ›¦ä¸€æ ·çš„ç½®èº«äº‹å¤–ã€‚è‡³å°‘å·²ç»ç™½é€ä½ ä»¬é¾™æ™¶åšæ­¦å™¨äº†ï¼ˆä¸ç„¶çœŸçš„0èƒœç®—ï¼‰ï¼Œå‡­ä»€ä¹ˆè¿˜è¦æŠ›å¼ƒç«‹åœºï¼Œåƒé‡Œè¿¢è¿¢å¿ç€æ°´åœŸä¸æœè·‘å»åŒ—å¢ƒæ‰“å‰é”‹ï¼ŒæŸå…µæŠ˜å°†ï¼Œè®©æ­»æ•Œç‘Ÿæ›¦åœ¨èƒŒååæ”¶æ¸”åˆ©ï¼Ÿè™½è¯´å¤œç‹æ˜¯äºŒä¸«æ”¶å‰²çš„ï¼Œä½†æ‰“å‰é”‹çš„æ˜¯å¤šæ–¯æ‹‰å…‹éª‘å…µã€æ©æŠ¤æ’¤é€€çš„æ˜¯æ— å¢è€…å†›å›¢ã€é¾™ç„°AOEå…¨åœºã€è¿˜æŠŠå¤œç‹å‡»è½åˆ°åœ°é¢ï¼Œè¿™ç»å¯¹æ˜¯åˆæœ‰åŠŸåŠ³åˆæœ‰è‹¦åŠ³å§ï¼Ÿç„¶è€Œæˆ˜åç‹‚æ¬¢æœ€å°½å…´çš„éƒ½æ˜¯åŒ—å¢ƒäººï¼Œç°è™«å­å’Œå¼¥æ¡‘é»›è¿™æ ·çš„å¤–æ¥è€…æ ¹æœ¬æ— æ³•èå…¥ï¼Œä¹”æ‹‰å’Œå¤šæ–¯æ‹‰å…‹é¦–é¢†æ­»äº†ï¼Œé¾™å¦ˆä¸€ä¸ªäººååœ¨é‚£é‡Œï¼Œæ²¡æœ‹å‹æ²¡å®¶äººï¼Œå¿ å¿ƒè€¿è€¿çš„éƒ¨ä¸‹è¿˜æ­»äº†ä¸€åŠï¼Œçœ‹ç€é›ªè¯ºè¢«ç°‡æ‹¥è¢«æ­Œé¢‚ï¼Œå¿ƒé‡Œå­¤ç‹¬ + ç‹ä½å²Œå²Œå¯å±ï¼Œå¿ƒæ€å¯ä¸æ˜¯å°±è¦å´©å—ã€‚ é¾™å¦ˆæ€»æ˜¯è¢«è¯Ÿç—…é ç”·äººå’Œå¼€æŒ‚ä¸Šä½ï¼Œä¸­åæœŸä¹Ÿç¡®å®å¼€å§‹å˜å¾—æœ‰ç‚¹ç‹ æ¯’å’Œå¼ºç¡¬ã€‚ä½†æ˜¯å°æ¶é­”è¯´å¾ˆå¯¹å•Šï¼Œâ€å›ä¸»å°±æ˜¯è¦æœ‰ä¸€å®šå¨æ…‘åŠ›â€ã€‚ä¹Ÿæ˜¯æƒæ¸¸é­…åŠ›ä¹‹æ‰€åœ¨å•Šï¼Œä¸ºä»€ä¹ˆéè¦å®Œç¾äººè®¾ã€‚ æœ€åè¯´è¯´ç‰‡å°¾ååˆ†é’Ÿå¯¹æ ‡â€äº®å‰‘â€çš„åŸæ¥¼æˆã€‚ ä¸€å¼€å§‹å¾ˆå¤šäººè¯´æåˆ©æ˜‚åæœŸæ™ºå•†ä¸‹é™æˆ‘è¿˜ä¸ä»¥ä¸ºç„¶ï¼Œè¿™é›†çœŸçš„æ²¡æ³•æ´—äº†ã€‚éƒ½ä»€ä¹ˆæ—¶å€™äº†ï¼Œè¿˜æƒ³ç€ç”¨çˆ±æ„ŸåŒ–ç‘Ÿæ›¦å‘¢ï¼Ÿï¼Ÿå¥¹éƒ½å§”èº«æ”¸ä¼¦ã€æ´¾åˆºå®¢æ€Jamieã€é«˜åˆ©è´·é›‡ä½£é»„é‡‘å›¢ã€æ”¾å¥¹ç—›æ¨çš„å›ä¸´è´±æ°‘è¿›çº¢å ¡äº†ï¼Œè¿™æ˜æ˜¾æ˜¯å­¤æ³¨ä¸€æ·ä¸æ­»ä¸ä¼‘çš„æ¶åŠ¿ã€‚è€Œä¸”åˆšåˆšå¹²æ­»ä¸€åªé¾™ï¼Œè¿˜æŠ“äº†å¹²éƒ¨å¼¥æ¡‘é»›ï¼Œåˆé€¢é¾™å¦ˆå¤§å†›åŠæ­»åŠæ®‹ï¼ŒåŒ—å¢ƒæ´å†›è¿˜åœ¨èµ¶è·¯ã€‚è¿™ä¼šå„¿æ‰‹æŒäººè´¨ç«™åœ¨åˆ¶é«˜ç‚¹ï¼Œæ•°æ¶å·¨å¼©éšæ—¶å‡†å¤‡å± é¾™ï¼Œç®€ç›´ä¸è¦æ›´å¾—æ„ã€‚è¿™ç§åœºé¢ä¸‹æåˆ©æ˜‚è¿˜æ€¼åˆ°åŸå¢™æ ¹å„¿æ¥å˜´ç‚®åŠé™ï¼Œexcuse me ??? è¿˜æ˜¯ç§‘æœ¬å¤´è„‘æ¯”è¾ƒæ¸…é†’ã€‚ã€‚ è¿™é›†è¢«ä¼ æ˜¯å²ä¸Šæœ€å·®ä¸€é›†çœŸæ˜¯ä¸å†¤æ‰ã€‚å”¯ä¸€å–œæ¬¢çš„æˆä»½å°±æ˜¯ä¸‰å‚»å’ŒçŒç‹—çš„äº’åŠ¨äº†ã€‚ å½’æ ¹ç»“åº•ã€ŠæƒåŠ›çš„æ¸¸æˆã€‹å¹¶ä¸æ˜¯ã€Šå†°ä¸ç«ä¹‹æ­Œã€‹å•Šã€‚å‰ä¸‰é›† A Song of Ice and Fire æ˜¯ä¸ºäº†å¹³è¡¡é¾™ç‹®æˆ˜åŠ›ï¼Œåä¸‰é›†äº‰å¤ºé“ç‹åº§æ‰æ˜¯çœŸçš„ç‚¹é¢˜ Game of Throne å•Šã€‚çœ‹ç¬¬äº”é›†é¢„å‘Šé‡Œæ”¸ä¼¦çš„ä¸€è„¸éœ‡æƒŠå¤–åŠ èƒŒæ™¯é¾™åŸå£°ï¼Œä¼°è®¡Drogonè¦ç©¿ç›”ç”²ï¼Œä¸ä¼šå†ç»™å·¨å¼©å½“æ´»é¶å­äº†ã€‚ å¸Œæœ›ç¼–å‰§æœ€åä¸¤é›†èƒ½ç»™åœ†å¥½äº†ï¼Œåˆ«çƒ‚å°¾ã€‚ä¸‹ä¸€éƒ¨è¿™ç§ç°è±¡çº§çš„ç”µè§†å‰§ä¸çŸ¥é“è¦åˆ°ä»€ä¹ˆæ—¶å€™äº†ã€‚","link":"/zh-cn/GoT-S8-E4/"},{"title":"æ­£åˆ™è¡¨è¾¾å¼ä¸grep, sed, awkçš„åŸºæœ¬ç”¨æ³•","text":"1. Regular Expression æ­£åˆ™è¡¨è¾¾å¼æ­£åˆ™è¡¨è¾¾å¼å¸¸ç®€ç§°ä¸ºregexã€‚ä¸€ä¸ªregular expressionå¸¸è¢«æˆä¸ºä¸€ç§patternï¼ˆæ¨¡å¼ã€æ¨¡æ¿ï¼Ÿï¼‰ï¼Œç”¨æ¥æè¿°å’ŒåŒ¹é…ä¸€ç³»åˆ—ç¬¦åˆæŸç§è§„åˆ™çš„å­—ç¬¦ä¸²ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åŸºæœ¬ç”¨æ³•ã€‚ 1.1 Quantifier æ•°é‡é™å®šç¬¦æ³¨æ„åŒºåˆ«é€šé…ç¬¦â€˜*â€™ Char Description Example ? è¡¨ç¤ºå‰é¢çš„tokenå‡ºç°0æˆ–1æ¬¡ app?le å¯åŒ¹é… aple(0æ¬¡)ã€appleï¼ˆ1æ¬¡ï¼‰ + è¡¨ç¤ºå‰é¢çš„tokenå‡ºç°1æˆ–næ¬¡ go+gle å¯åŒ¹é… gogleï¼ˆ1æ¬¡ï¼‰ã€googleã€gooogleâ€¦â€¦ï¼ˆnæ¬¡) * è¡¨ç¤ºå‰é¢çš„tokenå‡ºç°0æˆ–næ¬¡ ali(ba)* å¯åŒ¹é… aliï¼ˆ0æ¬¡ï¼‰ã€alibaï¼ˆ1æ¬¡ï¼‰ã€alibabaâ€¦â€¦ï¼ˆnæ¬¡ï¼‰ {n} è¡¨ç¤ºå‰é¢çš„tokenå›ºå®šå‡ºç°næ¬¡ o{2} å¯åŒ¹é…zooã€moonã€bookâ€¦â€¦ä½†æ˜¯Bobä¸è¡Œ {n,} è¡¨ç¤ºå‰é¢çš„tokenè‡³å°‘å‡ºç°næ¬¡ {1,}ç›¸å½“äº+ï¼›{0,}ç›¸å½“äº* {m,n} è¡¨ç¤ºå‰é¢çš„tokenå‡ºç°mï½næ¬¡ As above. Note: é»˜è®¤ä¸ºgreedy matchingï¼ŒåŒ¹é…æœ€é•¿çš„å­—ç¬¦ä¸²ï¼›å½“?æ·»åŠ åœ¨å…¶ä»–ä»»æ„quantifieråé¢æ—¶ï¼Œè¡¨ç¤ºlazy matchingï¼ŒåŒ¹é…æœ€çŸ­çš„å­—ç¬¦ä¸²ã€‚e.g.h.+l matches &#39;hell&#39; in &#39;hello&#39;, greedy.h.+?l matches &#39;hel&#39; in &#39;hello&#39;, lazy. 1.2 å…¶ä»–å¸¸ç”¨ç¬¦å· Char Description Comments ^ å­—ç¬¦ä¸²starting position ^a å¯åŒ¹é…â€™aliasâ€™ï¼Œline-based toolsä¸­è¡¨ç¤ºæ¯ä¸€è¡Œçš„å¼€å¤´ $ å­—ç¬¦ä¸²çš„ending position n$ å¯åŒ¹é…â€™qq.cnâ€™ï¼Œline-based toolsä¸­è¡¨ç¤ºæ¯ä¸€è¡Œçš„ç»“å°¾ () grouping ali(ba)*ä¸­â€˜baâ€™æ˜¯ä¸€ä¸ªtokon . é™¤äº†\\n(new-line)ä¹‹å¤–çš„ä»»æ„å•ä¸ªå­—ç¬¦ å¸¸é…åˆquantifier.+è¡¨ç¤º1-nä¸ªä»»æ„å­—ç¬¦ \\ è½¬ä¹‰ç¬¦ è½¬å³è¾¹ç‰¹æ®Šå­—ç¬¦ä¸ºå®ƒæœ¬èº«\\^=^ï¼›è½¬å³è¾¹æ™®é€šå­—ç¬¦ä¸ºç‰¹æ®Šæ„ä¹‰\\n=newlineï¼›Highest priority | Boolean OR gr(e|a)y åŒ¹é… â€˜greyâ€™å’Œâ€™grayâ€™æ³¨ï¼šç”±äºhexoå¯¹markdownçš„è§£æè¿˜ä¸å¤Ÿå®Œå–„ï¼Œæ­¤å¤„vertical baræ— æ³•æˆåŠŸè½¬ä¹‰ï¼Œæ€»æ˜¯æ‰“ä¹±è¡¨æ ¼ï¼Œæ•…è¡¥å……åœ¨å¼•ç”¨é‡Œã€‚ 2. grep(Globally search a Regular Expression and Print)2.1 Syntaxgrep [ opt ]... pattern [ file/dir/stdin ]...æ”¯æŒä»¥ä¸‹ä¸‰ç§RegExå¼•æ“-E POSIXæ‰©å±•æ­£åˆ™è¡¨è¾¾å¼ï¼ŒERE (ä½¿ç”¨é™¤äº†*ä¹‹å¤–quantifieræ—¶ï¼Œ|æ—¶)-G POSIXåŸºæœ¬æ­£åˆ™è¡¨è¾¾å¼ï¼ŒBREï¼ˆé»˜è®¤ï¼‰-P Perlæ­£åˆ™è¡¨è¾¾å¼ï¼ŒPCRE 2.2 Options &amp; Arguments Options Description -a â€“text å°†binary-fileä½œä¸ºæ–‡æœ¬æ¥è¿›è¡ŒåŒ¹é… -b â€“byte-offset åœ¨è¾“å‡ºè¡Œçš„è¡Œé¦–æ˜¾ç¤ºoffset -c â€“count æ‰“å°æ¯ä¸ªæ–‡ä»¶åŒ¹é…åˆ°çš„è¡Œæ•° -i â€“ignore-case å¿½ç•¥å¤§å°å†™ -n â€“line-number æ˜¾ç¤ºåŒ¹é…æ–‡æœ¬æ‰€åœ¨è¡Œçš„è¡Œå· -v â€“invert-match, åå‘åŒ¹é…ï¼Œè¾“å‡ºä¸åŒ¹é…è¡Œçš„å†…å®¹ -r â€“recursive é€’å½’åŒ¹é… -A n afterï¼Œé™¤äº†åˆ—å‡ºåŒ¹é…è¡Œä¹‹å¤–ï¼Œè¿˜åˆ—å‡ºåé¢çš„nè¡Œ -B n beforeï¼Œé™¤äº†åˆ—å‡ºåŒ¹é…è¡Œä¹‹å¤–ï¼Œè¿˜åˆ—å‡ºå‰é¢çš„nè¡Œ â€“color=auto å°†è¾“å‡ºä¸­çš„åŒ¹é…é¡¹è®¾ç½®ä¸ºè‡ªåŠ¨é¢œè‰²æ˜¾ç¤º ç‰¹æ®Špattern Description [:digit:] 0-9 [:upper:] A-Z [:lower:] a-z [:alpha:] A-Z, a-z [:alnum:] 0-9, A-Z, a-z [:punct:] punctuation symbol [:graph:] [:alnum:]+[:punct:] [:blank:] [space] and [Tab] [:space:] whitespace: tab, newline, vertical tab, form feed, carriage return, and space. [:cntrl:] æ§åˆ¶æŒ‰é”®ï¼ŒåŒ…æ‹¬ CR, LF, Tab, Del..ç­‰ [:print:] [:alnum:]+[:punct:]+space [:xdigit:] Hexadecimalï¼š0-9, A-F, a-f Note: ä½¿ç”¨è¿™äº›special patternæ—¶è¦é¢å¤–åŠ ä¸€å¯¹ä¸­æ‹¬å·â€œ[]â€ã€‚ [^]å½“^åœ¨ä¸­æ‹¬å·é‡Œæ—¶ï¼Œè¡¨ç¤ºæ’é™¤ 2.3 Example1234$ grep '[[:lower:]]' file1 #åŒ¹é…å«æœ‰'a-z'çš„è¡Œã€‚ åŒé‡ä¸­æ‹¬å·ã€‚$ grep '[^sci]' file2 #æ’é™¤sã€cã€iä¸‰ä¸ªå­—ç¬¦ï¼Œè€Œéâ€˜sciâ€™å­—ç¬¦ä¸²ã€‚$ grep -E '^(w|h)' file3 #åŒ¹é…ä»¥'w'æˆ–'h'å¼€å¤´çš„è¡Œã€‚ä½¿ç”¨'|'éœ€åŠ ä¸Š'-E'é€‰é¡¹ã€‚$ grep -E 'w&#123;3&#125;' file3 #åŒ¹é…å«æœ‰'www'çš„è¡Œã€‚ ä½¿ç”¨quantifieréœ€åŠ ä¸Š'-E'é€‰é¡¹ã€‚ 3. sed(Stream editor)sedæ˜¯ä¸€ä¸ªéäº¤äº’å¼ï¼ˆnon-interactiveï¼‰å‘½ä»¤è¡Œæ–‡æœ¬ç¼–è¾‘å™¨ï¼Œç”¨äºè¿‡æ»¤ã€è½¬æ¢æ–‡æœ¬ï¼Œé…åˆscriptå’Œæ­£åˆ™è¡¨è¾¾å¼æ¥åŒæ—¶å¤„ç†ä¸€ä¸ªæˆ–å¤šä¸ªæ–‡æœ¬æ–‡ä»¶ã€‚ 3.1 Syntaxsed [ opt ]... {script} [ file/stdin ]... 3.2 Options opt Description -n â€“silent å®‰é™æ¨¡å¼ï¼Œåªæ‰“å°å—å½±å“çš„è¡Œã€‚é»˜è®¤æ‰“å°stdinçš„å…¨éƒ¨è¡Œã€‚ -e â€“expression æ·»åŠ script -f â€“file æ–‡ä»¶ä¸­çš„å‘½ä»¤ -r ä½¿ç”¨EREï¼Œé»˜è®¤ä¸ºæ ‡å‡†æ­£åˆ™è¡¨è¾¾å¼ -i ç›´æ¥ä¿®æ”¹è¾“å…¥æ–‡ä»¶å†…å®¹ï¼Œè€Œä¸æ˜¯æ‰“å°åˆ°stdout -s â€“separate ä»¥filesä¸ºåˆ†å‰²ï¼Œè€Œä¸æ˜¯fileçš„æ¯ä¸€è¡Œã€‚æ­¤æ—¶stdinä¸ºå¤šä¸ªæ–‡ä»¶ã€‚ 3.3 Script syntax[addr]X[opts] éœ€ç”¨-eé€‰é¡¹å¼•å¯¼ï¼Œæˆ–ä½œä¸ºç¬¬ä¸€ä¸ªnon-option argumentï¼›ä¹Ÿå¯å†™åœ¨fileé‡Œç”¨-fé€‰é¡¹å¼•å¯¼ã€‚ scriptå†…å¤šæ¡å‘½ä»¤å¯ç”¨semicolonâ€œ;â€åˆ†éš”ã€‚ å‘½ä»¤a, c, i,åä¸å¯è·Ÿâ€œ;â€ï¼Œæ•…muti-cmdæ—¶éœ€æ”¾åœ¨æœ€åä¸€ä¸ªã€‚ [addr] å•è¡Œ ï¼šn è¿ç»­å¤šè¡Œ ï¼šn1,n2 ä»n1è‡³n2ã€‚ è·³è·ƒå¤šè¡Œ ï¼šn~s ä»nå¼€å§‹ï¼Œä»¥sä¸ºåŸºæ•°è·³è·ƒã€‚å¦‚1~2å¥‡æ•°è¡Œï¼Œ2~2å¶æ•°è¡Œã€‚ Regex ï¼šç”¨æ­£åˆ™è¡¨è¾¾å¼å–å€ã€‚å¦‚/^foo/ä»¥â€œfooâ€å¼€å¤´çš„è¡Œã€‚ åé¢åŠ ä¸Šâ€œ!â€è¡¨ç¤ºæ’é™¤ X[opts] p print æ‰“å° é€šå¸¸é…åˆ-nå¼€å¯å®‰é™æ¨¡å¼ d delete åˆ é™¤ s substitute ä»£æ›¿ï¼ˆpatternå­—ç¬¦ä¸²ï¼‰ a append å‘åæ·»åŠ  i insert å‘å‰æ’å…¥ c change æ›´æ¢ï¼ˆæ•´è¡Œï¼‰ å…¶ä¸­s/regex/text/[flag]ï¼Œ[flag]å¯ä»¥æ˜¯gï¼ˆæ›¿æ¢è¡Œå†…å…¨éƒ¨åŒ¹é…ï¼‰ï¼Œæ•°å­—nï¼ˆæ›¿æ¢è¡Œå†…ç¬¬nä¸ªåŒ¹é…ï¼‰ï¼Œpæ›¿æ¢æˆåŠŸåæ‰“å°ã€‚ å…¶ä¸­a, i, cå‘½ä»¤å¯åŠ backslashâ€œ\\â€æ¢è¡Œåå†å†™textã€‚ 3.4 Example123456#deleteä»¥â€œfooâ€å¼€å¤´çš„è¡Œï¼Œæ›¿æ¢æ‰€æœ‰è¡Œçš„ç¬¬3ä¸ªâ€œhelloâ€ä¸ºâ€œworldâ€$ sed â€™/^foo/d ; s/hello/world/3â€™ input.txt &gt; output.txt #æ›¿æ¢æ‰€æœ‰ä¸å«â€œappleâ€çš„è¡Œä¸ºâ€œhello thereâ€$ sed â€™/apple/!c\\hello thereâ€™ input.txt &gt; output.txt 4. awkæ–‡æœ¬å¤„ç†è¯­è¨€4.1 å·¥ä½œåŸç†æŒ‰è¡Œ(record)è¯»å–ï¼ŒæŒ‰ç©ºæ ¼ï¼ˆFSï¼‰è¿›è¡Œåˆ‡ç‰‡(field)ï¼Œå°†æ¯ç‰‡ä¿å­˜åœ¨å†…å»ºå˜é‡ä¸­ï¼Œ$1,$2,$3â€¦.ã€‚$0è¡¨ç¤ºå…¨éƒ¨ã€‚å¯ä»¥å¯¹å•ä¸ªç‰‡æ–­è¿›è¡Œåˆ¤æ–­ï¼Œä¹Ÿå¯ä»¥å¯¹æ‰€æœ‰æ–­è¿›è¡Œå¾ªç¯åˆ¤æ–­ã€‚ 4.2 Syntaxawk [ -F fs ] [ -v var=value ] [ &#39;script&#39; | -f scriptfile ] [ file ... ] 4.3 Builtin Var var content FS field separator é»˜è®¤ä¸ºä¸€ä¸ªç©ºæ ¼ RS record separator é»˜è®¤ä¸ºä¸€ä¸ª\\n NF number of field é»˜è®¤ä¸ºç‰‡æ®µä¸ªæ•° NR number of record é»˜è®¤ä¸ºæ–‡ä»¶è¡Œæ•° FNR number of record in each file OFS è¾“å‡º field separator ORS è¾“å‡º record separator ARGC å‘½ä»¤è¡Œargumentsä¸ªæ•° ARGV å‘½ä»¤è¡Œargumentsæ•°ç»„ FILENAME å½“å‰è¾“å…¥æ–‡ä»¶å 4.4 å¸¸ç”¨awk &#39;{print $2}&#39; æ‰“å°ç¬¬äºŒfieldã€‚ å’Œ cut -dâ€˜ â€™ -f2æå¤´å»å°¾çš„ç”¨æ³•å¼‚æ›²åŒå·¥ 5. Reference[1]é­é•‡åªï¼šLinuxä¹‹awkè¯¦è§£[2]EmanLee: awk ç”¨æ³•ï¼ˆä½¿ç”¨å…¥é—¨ï¼‰","link":"/zh-cn/SHELL_grep_sed_awk/"},{"title":"It's always the little things","text":"This post is to test local audio.Itâ€™s always the little things var ap = new APlayer({ element: document.getElementById(\"aplayer-RbrLQCkl\"), narrow: false, autoplay: false, showlrc: false, music: { title: \"It's always the little things\", author: \"éƒ­é¡¶, å±±å½¢ç‘ç§‹\", url: \"/audio/LittleThingséƒ­é¡¶Yamagata.mp3\", pic: \"/audio/LittleThingséƒ­é¡¶Yamagata.jpeg\", lrc: \"\" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); Itâ€™s always the little things That seep into your soul How can I lie here still awake Something inside of me is telling me to go Iâ€™ve been waiting to tell you so Mmm Take me home and make it right I long to see the light Iâ€™m waiting for a sign And I donâ€™t know if we should part What does it mean if we give up How can I still feel this love for all that weâ€™ve been through And I donâ€™t know what I should say Is it the time to walk away Should we let go or should we stay And find a way through Mmm Take me home and make it right Can you still see the best of me Or am I falling out of sight Itâ€™s always the hardest thing To listen to your soul When something is telling you to go A part of you wants to fight And part just fly away How I long to tell you so And I donâ€™t know if we should part What does it mean if we give up How can I still feel this love For all that weâ€™ve been through And is it time to walk away Should we let go or should we stay How do you know when somethingâ€™s too far gone to get back to Mmm Take me home and make it right Can you still see the best in me Or am I falling Am I falling Am I falling out of sight","link":"/zh-cn/test-music/"},{"title":"â€œç”·ä¸å¥³â€","text":"var ap = new APlayer({ element: document.getElementById(\"aplayer-ujRAswkD\"), narrow: false, autoplay: true, showlrc: false, music: { title: \"14. ì •ì‚¬ [ ë‚¨ê³¼ ì—¬ OST]\", author: \".\", url: \"amaaw.mp3\", pic: \"/zh-cn/amaaw/amaaw.jpeg\", lrc: \"\" } }); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); ğŸŒ™ æ™šå®‰","link":"/zh-cn/amaaw/"},{"title":"å¹³å‡¡æ–™ç†ï¼šçš®å¾ˆè„†çš„ç…§çƒ§é¸¡è…¿è‚‰","text":"æµ‹è¯•iframeè§†é¢‘æ’å…¥ã€‚å°±æ”¾vvçš„å¹³å‡¡æ–™ç†å§ã€‚ youtube tag 1&#123;% youtube 4IOI-98_d9c %&#125; çš®å¾ˆè„†çš„ç…§çƒ§é¸¡è…¿è‚‰ on YouTubeã€‚ iframe 1&lt;iframe width=\"360\" src=\"https://www.youtube.com/embed/4IOI-98_d9c\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt; ç¬¬ä¸€æ¬¡å°±å¤§æˆåŠŸå¤§æ»¡è¶³çš„è’œé¦™ç‰›æ’é¥­ on Bilibiliã€‚ çœ‹é¥¿äº†ã€‚","link":"/zh-cn/test-video/"},{"title":"CS5052 Data-Intensive Systems - Revision Note","text":"Revision Checklist Week 1 - 2 slides Week 3 - 11 papers Previous Exams Examinable Topics Core Papers Parallel Processing 1. MapReduce2. RDD (Resilient Distributed Datasets)3. GFS (Google File System) Storage and Databases 4. CAP and BASE theorem5. Amazonâ€™s Dynamo Graph and Stream Processing 6. Pregel Resource Managers 7. Googleâ€™s Borg8. Mesos Cloud Engineering 9. Performance Variation and Predictability in Public IaaS Clouds 10. HotSpot Edge / Decentralised 11. Incremental deployment and migration of geo-distributed situation awareness applications in the fog Machine Learning and Systems 12. Learning Scheduling Algorithms for Data Processing Clusters Distributed Systemsa collection of independent computers that appears to the user as a single coherent system. Tanenbaum, 2006 Complex:components are autonomous and collaborative. Design issues: Heterogeneity. Network/hardware/OS/languages are heterogenous; use standard protocols (HTTP) Openness, taken for granted at networking level but not universal at component level; determines whether the system can be extended or re-implemented in various ways; new resource sharing service can be added by clients; key interfaces published; uniform communication mechanisms; Security, might be mutually incompatible security policies, more ways of being attacked comparing to centralised systems; Scalability, size (add more resource), distribution (geo-disperse without degrading), manageability. Scaling up (upgrade resources), scaling out (add new). Techniques: hiding latency (avoid blocking on remote), distribution, replication Failure handling; usually partial, detection difficult and can cascade; must provide high degree of availability. Techniques: detecting/masking failures (retransmitting), design tolerating client, recovery policy, redundancy (database replication) Transparency, ideally user not aware distribution, practically impossible (parts are independently managed and network latency); usually make users aware so that can cope with problems (node crash); Types: hide access/location/migration/replication/concurrency/failure/scaling Quality of Service, reflects the systemâ€™s ability to deliver services reliably with a response time/throughput acceptable to users. (e.g. if quality falls then sound/video so degraded that impossible to understand) Centralised (2-tier) and Decentralised (N-tier/P2P) distributed computing systems Homogeneous Cluster computing: largly the same, physically close, highly connected, ONE admin domain Heterogeneous Grid computing: no assumptions about hardware/OS/network, MULTIPLE domains Cloud computing: elastic capacity, pay-per-use, on-demand self service, resources are virtualised Structure of Distributed SystemsThe structure of distributed systems top to bottom: Applications/Services Middleware Operating System Computer and Network Hardware Application/Service (Layer 1) has client-server architecture. A server for a distributed database may act as a client, forwarding requests to different file servers. Three levels: UI, processing, data. In a 2-tier client-server (centralised) architecture, singple logical server mounting indefinite number of clients. Thin-client model, only presentation layer implemented on client, all others on server. simple to deploy and manage the clients heavy processing load on both server and network Fat-client model, some/all of the application processing is locally executed on client. suitable when the capabilities of client system are known in advance more complex than thin, expecially for management of new clients less network traffic new versions have to be installed on all clients In a Multi-tier client-server (decentralised: vertical distribution) architecture, seperate processes execute on different physical machines/servers. Alleviate problems with scalability, potentially offer higher performance. In a peer-to-peer (decentralised: hotizontal distribution) architecture, seperate shares of a complete set of data are operated on several logically-equivalent clients/servers. Attr: processes on peers are all equal; interactions are symmetric; each process acts as both client and server PRO: Resilience; Scalability CON: no-oneâ€™s in charge; each peer only knows limited info Middleware (Layer 2) is the software that provides useful building blocks, manages the diverse components (involving diff lang/processors/protocols) and ensure they can communicate and exchange data. OS and hardware (Layer 3 &amp; 4) are platform. Some trivial points: Virtual Machine: VMWare, XenSource Vittual Storage: GFS, HDFS Big data: high-volume, high-velocity, high-variaty; must be converted to knowledge and understanding in order to stimulate scientific discoveries, industry innovations and economic growth. Cloud Computing To developers: no upfront infrastructure investment reduced operating costs through utility pricing elastic on-demand architecture high service availability ease of use To providers: exploit existing data-centre capacity take advantage of economies of scale (available to purchasers of extremely large volumes of hardware and network capacity) Enabled technologies: Virtualization: abstract physical infrastructure, isolated guest OS co-existing on the same computer (see figure below) Virtual storage: GFS, HDFS Service-Oriented Architecture (SOA): REpresentational State Transfer(REST); develop applications using existing services, make applications available as services or both Tight Coupling: components difficult to understand in isolation, have many interdependency, difficult to maintain and re-use, cascade changes Loose Coupling: components independent; easier maintainance and re-use Ultility Computing: pay-per-use, on-demand Different types of cloud: IaaS (infrastructure), Amazon Web Services deliver raw computer infrastructure vendors manage networking, hard drives, OS etc. PaaS (platform), Microsoft Azure deliver an application framework hosted framework user can built applications on SaaS (software), Google Mail deliver applications as a service Terminology Machine Images: a template that contains software configuration Instances: different instance types are essentially different hardware archetype Regions: geo areas of the world which instances can be deployed in, each region contains multiple Availability Zones (AZ), low latency network within the same region Elastic Compute Cloud (EC2): a service providing resizable computing capacity in Amazonâ€™s data centres Simple Storage Service (S3): Internet storage, REST and SOAP interfaces for app dev Glacier: low-cost storage service, optimized for infrequently accessed data, e.g. retrievsl times of several hours (achiving research/scientific data etc.) Metrics: CPU utilization, latency, request counts. Some custom app/sys metrics: memory usage, transaction volume, error rate Cloud vendors commonly provide Software Development Kit (SDK) for different languages, hide low-levels from end-users with abstract interface. SummaryCloud Computing provides resizable computing capacity that enables users to build and host applications in a data centre. computing as a utility elastic capacity pay-per-use self service interface resources virtualized","link":"/zh-cn/CS5052-revision/"},{"title":"MT5761 Statistics Modeling - Revision Note","text":"Questions and solutions in previous exam paper. Numbers in square brackets indicate marks. 1. lm, glsCorr, glsCorrVar [14](a) Describe predict power of a lm model. Comment model performance over some covariates e.g. time! [2][1] Reasonably poor; fairly low $R^2$ 0.2; poor agreement between observed and fitted [1] Prior to A are underestimated, between A to B are overestimated, post B are underestimated. (b) Three assumptions of lm model; Descibe validity of them. [3][1] Normality assumption, appears to be violated, Shapiro-Wilk $H_0$ NORMAL [1] Constant error variance assumption, no evidence violated, Bresh-Pagan test $H_0$ CONSTANT [1] Independence assumption, clearly violated, correlation shown in $\\text{acf}$ plot &amp; Durbin-Watson test $H_0$ INDEPENDENT, test stats less than 2 (c) Thinkâ€™bout source of data, other reason of correlation. [1]Realistic reason. (d) Describe mean-var relationships underlying BP test, contrast with that assumed by glsVar. [2][1] Breusch-Pagan Test use $r_i^2 = \\alpha_0 + \\alpha_1x_i + \\gamma_i$ to determine the extent of agreement between residual variance and vars $x$, where $r_i^2$ are squared residuals, $\\alpha_0$ and $\\alpha_1$ are estimated in the model. [1] BP test assume the test statistics $NR^2\\sim \\chi^2_p$ in this case degree of freedom p = 1. GLS model assume $r^2_i \\sim N(0,\\sigma^2|\\hat{y_i}|^{2m})$ or $N(0,\\sigma^2e^{2my_i})$ (e) What do BP test and glsVar suggest about the mean-var relationship? [3][1] BP test checked(not constant): No evidence for a linear relationship with squared residuals. [1] glsVar: There appears to be a non-zero power-based relationship. [1] AIC smaller when the power coefficient is fitted, and zero is not a plausible in the gls model. (f) Conclude which model is most defensiable. [3][1] Overall Conclusion: There is no/strong evidence for a change of RespVar over ExpVars. [1] Best fitting model (based on AIC [1], itâ€™s glsCorrVar) exhibits a large/small p-value (0.05) for the relationship. The models which inappropriately ignore something concludes ExpVar is (not) significant. 2. glm for Poisson(OD) [11](a) Relationship impied by glmPois model on RAW and LINK scale. Which is suitable. [3][1] glmPois assumes a nonlinear relationship between RespVar and ExpVars and a linear relationship on log/sqrt scale. [1] Not good for this data; [1] model allow monotonic relationship but function seems to need inflection points. (b) Mean-Var relationships underpins glmPois and glmPoisOD. Which is more realistic. [3][1] glmPois assume fitted mean and residual var are equal. [1] glmPoisOD assume residual var is proportional to fitted mean. $\\text{var} = \\phi \\lambda$ [1] In this case, latter is more realistic since estimate of dispersion parameter $\\hat\\phi=399$ is much larger than 1. (c) Contrast conclusions of glmPois ,glmPoisOD andglsCorrVar, which is most defensiable. Additional methods to improve. [3][1] glmPois and glmPoisOD suggest strong evidence for negative/positive Resp-Exp relationship. â€‹ glsCorrVar suggest this not well-evidenced, could be sampling variability alone. [1] would use glsCorrVar to base my conclusion on. Because although looks nonlinear (even Poisson-based are barely linear on raw scale), the non-constant error variance and correlation are modelled in errors. [1] smoother based function(splines) to improve the relationship? GEE(Generalized Estimating Equation) much like GLMs but allow non-independence. (d) How to investigate the effects of XXX or XXX on RespVar? If RespVar available at both. [2][2] e.g. Interactions, piecewise linear models, time as a factor variable. 3. glm for Binomial [15](a) Describe fit.logit , including RespVar, linear predictior, random component, link function. [4][1] Linear predictor [1] Link function [1] Intercept and Error term [1] Other Î² parameters Assuming $y_i \\sim Binomial(n_i,p)$ , where $y_i$ is the no. of observation with XXX out of the $n_i$ observations, and $p$ is the probability of XXX (being caught with fish in stomach), also the RespVar of the model, varying over ExpVars.$$p_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}} + \\epsilon_i\\$$Linear predictor is obtained by transforming RespVar by the Link function, which is$$g(p_i)=\\log{(\\frac{p_i}{1-p_i})}=\\eta_i=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4x_4+\\beta_5x_5$$$\\beta_0$is the intercept parameter (baseline), represents $\\text{female, &lt;2.3m}$ and $\\text{george}$. $\\beta_1$is the coefficient for male (compared with female) $\\beta_2$is the coefficient for &gt; 2.3m (compared with &lt; 2.3m) $\\beta_3$is the coefficient for hancock (compared with george) $\\beta_4$is the coefficient for oklawaha (compared with george) $\\beta_5$is the coefficient for trsfford (compared with george) $\\epsilon_i$ is the random component - the binomial error term (b) Odds of Binomial GLM [2]$$\\text{Odds} = \\frac{p(\\text{success})}{p(\\text{failue})} = \\frac{p_{it}}{1-p_{it}}= e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_3+\\beta_4x_4+\\beta_5x_5}$$ (c) Calculate odds use fit.logit.best [2]$$\\text{odds(small,trafford)} = e^{\\beta_0+\\beta_4}=e^{-0.1218-1.3733} = 0.2242262$$ (d) Multiplicative effect between odds for different levels [1][1] For baseline level 0 compared with non-base level n, the odds of presence(success) vs absence(failure) are estimated to change by a factor $e^{\\beta_n}$ (e) Explain Deviance [4][1] Deviance provides a measure of discrepancy between fitted model and Saturated model. The smaller the D, the better the model.$$D = 2[l(\\hat\\beta_{sat},\\phi) - l(\\hat\\beta,\\phi)]$$[1] If model correct, $D \\sim \\chi^2_{n-p-1}$, n observations, p predictors â€‹ A $\\chi^2$ test ($H_0$ model is correct) will give a large p-value for a good fitting. [1] $\\chi^2$ approximation of D often poor for Binomial GLMs [1] Computing $D$ involves $\\phi$, so if itâ€™s unknown we canâ€™t use the result. (f) adjustment to raw residuals and why [2][1] Adjustment.$$ \\text{Pearson residuals} = \\frac{\\text{raw residual}}{\\hat{\\text{SD}}}=\\frac{y-\\hat{y}}{\\sqrt{\\text{Var}(y)}}$$ [1] Why. Weâ€™d like to see no patterns in Pearson residuals if Mean-Var relationship is appropriate. 4. glm for Multinomial [10](a) Nominal and Ordinal [1]Nominal: Response value categories has no natural order. e.g. Ordinal: The matter of response matters. e.g. (b) Assumptions for multinomial GLM fit.mult [3][1] Independent observations from Multinomial distribution. [1] Linear relationship with covars, on (cumulative) log odds scale. [1] IIA : Independence from Irrelevant Alternatives. Assuming the odds of one outcome vs another does not dependent on what alternative outcomes are available. (c) Model Selection procedure to choose covariates[2][1] Fit models with all possible combinations of covariates; dredge [1] use fit criteria (e.g. AIC) to rank models. (d) Calculate response. Reptile; small; hancock[2]$$p_{ij} = \\frac{e^{\\eta_{ij}}}{1+\\sum_{k=2}^J e^{\\eta_{ik}}}$$ $\\eta_3 = -3.66588368 + 1.2431622 =-2.42272148$ $\\eta_2 = -0.09083394 -1.6583241= -1.74915804$ $\\eta_4 = -2.72380722 + 0.6952142 = -2.02859302$ $\\eta_4 =-1.57283851+ 0.8262891= -0.74654941$$$p_{ij} = \\frac{e^{-2.42272148}}{1+e^{-1.74915804}+e^{-2.42272148}+e^{-2.02859302}+e^{-0.74654941}}=0.04747016$$ (e) Assumption of a proportional odds model [2][1] Proportional odds Assumption (especially for ordinal!): â€‹ The slope of covar relationship is the same for for each outcome level. [1] To test the validity: â€‹ Fit a model that does not make it, use a model selection statistics to test between two. General Reasons for using GLM instead of LR: Response Var is not guaranteed to change linearly with Explainatory Vars; Response Var is naturally bounded by some range and LR predictions can produce values outside this range; Errors are unlikely to be normal with constant variance.","link":"/zh-cn/MT5761-revision/"},{"title":"MT4113 Statistic Computing - Revision Syllabus","text":"Revision syllabus for MT4113 by lectures/chapters. Fundamental programming principlesComputer (lecture 1) [x] Definition of a computer [x] Hardware, software [x] Programming languages: classification, generations, [x] compiled vs interpreted, which should be used when Some Definition Computer - A device that accept info and manipulates it according to a sequence of instructions to produce output Hardware - a physical computer equipment, including CPU, memory (Register, RAM), mass storage(ROM), I/O (keyboard,screen,printer) Software - computer program, a set of instructions operating the hardware. System sw(Linux, Win), App sw(SAS, Word), Programming sw(R) Programming LanguagesClassification by level Low - close to hardware, machine lang. High - far from hardware, close to natural lang. run on diff OS Generality - high lvl. more specialized in application Generation - 1st ~ 5th Speed/Efficiency: Low level generally faster(not always) Generations 1st: Machin lang. - Relatively few diï¬€erent instructions 2nd: Assembly lang. - Human-readable machine lang. 3rd: CPU-independent lang. - Fortran, C, Pascal, OOLs(C++, Java). most we use written in 3GL 4th: designed with a speciï¬c application; database query lang(SQL) graphical user interface(GUI) maths lang(Mathematica, Maple) stats lang(R, SAS) 5th: to solving problems given problem speciï¬cation; no need explicitly writting algorithm Prolog. Used is in artiï¬cial intelligence studies. Compiled vs InterpretedTwo approaches to turn langs into machine langs(must be done b4 canbe executed): Interpret: do it line by line when entering using an â€˜interpreterâ€™, run it straight way. Python, R Compile: written and save in file, turn all into machine code in one go using a â€˜compilerâ€™. Which and When Simple do once stuff - GUI, no reproducible trail more complex few times - 4GL within interpreter prodiction where efficiency important - 3GL and compile, or prototype in a 4GL plus slow bits in 3GL Algorithms (lecture 2) [x] Definition; important features; components of an algorithm [x] Examples, criticism of examples [x] Code and pseudocode [x] Control structures and vectorization in R Whatâ€™s An AlgorithmAlgorithm: an ordered sequence of unambiguous and well-deï¬ned instructions, performing some task and halting in ï¬nite time! Important features an ordered sequence unambiguous, well-defined(clear, do-able, canbe done without difficulty) perform tasks, nothing left out halt in finite time, so algorithm needs to be terminate Example boil a egg! Elements assigning values computation return values Control Structures Conditional exec - if, switch case Iteration (looping) - repeat a set of instr fixed number variable number (warning! potential infinite loop) Recursion - algorithms that call themselves Code and PseudocodeCode: instructions in computer lang that implements an algorithm. Pseudocode: instructions that are generic, informal, lang-unspecified, specify an algorithm. Always write out pseudocode b4 coding! VectorizationMuch faster than loops Some variants of apply - sapply, lapply, tapply, mapply, %by% Modularization and good programming practice (lect 2, 3, prac 2, 3) [x] Modular programming, definition, advantages, interface, passing by reference and value, best practices, encapsulation [x] Environments, scoping, dynamic lookup [x] Strategies for code design [x] Coding conventions and style [x] Testing and debugging. Types of code errors; how to build tests; debugging strategies. [x] Online collaborative programming tools: git repositories, clone, pull, push, commit. ModularizationModular programmingï¼split program into discrete, readable blocks of code so that each block does a small amount. Function Advantages avoid repeating easier to test esaier to update and propagate changes divide and conquer problems Module interfaces passing by value make copies, stored in separate location from original var changes inside function have no aï¬€ect on varâ€™s value outside inefficient but safer passing by reference directly using original var (location) changes to the variable within function aï¬€ect itsâ€™ value after the function run efficient but dangerous Most 3GLs allow both interfaces; one can specify interfaces for certain parameters in C; Base R does not allow passing by reference. Best practice/concept of Encapsulation pass everything required as an argument pass everything out using return() No global side effects within functions Environment (Workspace?!) Base environment (contains basic packages) Global environment (pkg loaded, var created), â€œresetâ€ every time reopen R Current environment (environment inside of a function) Scoping Name masking: looks up one level for undefined names(can rewrite predefined var, e.g. pi) R default to the version in most recently loaded pkg/func for same name func use of function independent of any previous (env wiped clean for new use) Whatâ€™s Dynamic Lookup It only matters when code is executed, rather than created. Design Strategies Top-down (rigid) and bottom-up (iterative) approaches flowcharts for planning and documentation outlines or pseudocode (breaking big task into manageable bits) Coding ConventionsBasically readablity and consistency &lt;â€“ for assignment, = for argument, == for boolean function comments listing purpose, I/O indentation and spacing around operators and after commas lines &lt;80 characters long meaningful names in consistent style Testing and DebuggingType of errors: syntax errors logic errors numerical errors (overflow/underflow/floating point problems) How to build test: what code should produce how does code behave with special case (0, nagetive, NA, gigantic) build test prior to building code (code is created to pass tests) Debugging strategy: Trial and error global arguments and step through func print() object as function progresses Computer arithmetic (lecture 4) [x] How numbers are stored on computers. Fixed point â€“ addition, multiplication, overflow. Floating point â€“ limitations, consequences of lack of accuracy. [x] Data size on computer. [x] Fixed and floating point numbers in R. [x] Living with floating point limitations â€“ strategies to diagnose and avoid problems. [x] Character storage. Fixed Point (32bits integer in R) addition å°±ç›´æ¥åŠ  multiplication å°±æ˜¯ shift and add overflow, åŠ èµ·æ¥è¶…ä½æ•°äº†ï¼ binary ä¹˜ä»¥ 2çš„næ¬¡å¹‚ = å°æ•°ç‚¹å³ç§»è¡¥é›¶ binary é™¤ä»¥ 2çš„næ¬¡å¹‚ = å°æ•°ç‚¹å·¦ç§»èˆä½ $n\\times 12 = n \\times (2^3) + n \\times (2^2)$ næ·»3ä¸ªé›¶ + næ·»2ä¸ªé›¶ Pro - results are exact, fast Con - limited applicability and range In R, 32 bit integer æœ€å¤§æ˜¯ $2^{31}-1$ Float Point (numeric in R)Represented as $S.F \\times B^E$ $S$ - Sign $F$ - Fraction, unsigned integer $B$ - Base, 2 for computer $E$ - Exponent, signed integer, shift left/right e.g. $-.110\\times 2^{1011} = -.000110 = -0.9375$ Limitations bits for $F$ (fraction) limits the accuracy bits for $E$ (exponent) limits the size $\\epsilon = 2^{1-f}$, the smallest positive number (interval) can be distinguished consequences of lack of accuracy rarely add up exactly, due to rounding err order of summation matters (canâ€™t overflow at the beginning) subtraction of two nearly equal numbers eliminates the most significant digits Pro - wide applicability, wider range Con - result not exact, slow In R defualt numeric 64 bits in total, 1 for sign, 11 for exponent, 53 for fraction (1 extra bit by a trick) largest $+.111â€¦_{53} \\times 2^{+1023} = 8.98 \\times 10^{+307}$ accuracy $\\epsilon = 2^{1-53} = 2.20\\times 10^{-16}$ Underows are set to 0, no warning Live with limit of Float Point re-work expression avoid overflow Normalize, scale by dividing change exact test to â€˜good enoughâ€™ test, from x == y to x - y &lt; e careful about order of operation double-precision(desperate) Many show a trade-off between accuracy and speed Character Storage as binary, determined by character set ASCII, 7-bit, 128 unique chars Unicode, 16-bit, 65536 chars Data Size bit: binary digit, smallest unit of storage on a machine, hold 0/1 byte: 8 bits - hold a single character word: natural data size of a computer, most 64 bits kilobyte (KB) $2^{10}$ = 1024 bytes megabyte (MB) $2^{20}$ = 1024 KB gitabyte (GB) $2^{30}$ = 1024 MB Computer-intensive statistics (lect 6, 7, 8)An understanding of how these methods work, ability to reproduce the algorithms and apply to case studies, pros and cons â€“ which to use when. [x] Overview of parametric and nonparametric methods for constructing tests and confidence intervals. Contexts include one sample, two sample, ANOVA, multiple regression. [x] Permutation methods â€“ test and interval [x] Randomization methods â€“ test and interval [x] Monte Carlo tests; Monte Carlo methods [x] Bootstrap â€“ nonparametric (including balanced bootstrap) and parametric. Percentile confidence intervals. Traditional Par and Non-par MethodTraditional Parametric method, specify PDF for population is known, $f(\\theta) \\sim N(\\mu, \\sigma^2)$; distribution of test statistic $T(x)$; Problems: result variant to transformation; bias correction. Traditional Non-parametric method, avoid fully specifying $f()$; e.g. Wilcoxon Signed Rank test: $f()$ is symmetric about some median m; $T(X)$ is specified Cons: Wilcoxon e.g. require to formulate $H_0$ in terms of median; Less powerful than a t-test when parametric assumptions met Computer-intensive Statistical MethodAvoid fully specifying either $f()$ and/or $T()$, e.g. Permutation-based approach Randomization-based approach Monte-Carlo-based version of t test (avoid $T()$, but not $f(\\theta)$) Par and Non-Par Bootstrap CI (avoid $T()$, $f(\\theta)$ depends) Permutation MethodsAssume $x_i$ is symmetric about $\\mu_0$. Given 12 obs., there are $2^{12} = 4096$ ways (permutaions). $p$-value is the proportion of that $|\\bar{X}|$ greater then observed $|\\bar{x}|$, how extreme the observed mean is. e.g. 48 out of 4096 are as extreme - p = 48/4096 = 0.012 Permutation tests pick $H-0$ and $T(X)$ under $H-0$ all permutation are equally likely calculate the $p$-value (proportion as extreme) free to choose whatever test statistic e.g. median, trimmed mean, t-stat, etc. readily extends to other simuation e.g. two-sample, ANOVA, multi regression Permutation CI Find the set of $\\mu_0$ such that $H_0: \\mu = \\mu_0$ is not rejected Pros - widely applicable; produce exact result; no specific distribution assumption for population; no analytic distribution of test statistic. Cons - each permutation must be equally likely (or prob is known); hard to program; can be prohibitively intensive for large sample size. Randomization MethodsRamdomization is like permutation, except that only random subset of all permutations. e.g. randomly assign $|x_i-\\mu_0|$ to each Compared with Permutation Methods Pros - all pros of it plus; easier to code; number of randomization is fixed rather than $2^{n}$ Cons - each permutation must be equally likely (or prob is known); results vary. Monte-Carlo TestsMonte-Carlo tests is a generalization of randomization test to include parametric distribution. Algorithm as Set population distribution $f()$ or $f(\\theta)$ and $H_0$ Simulate a dataset under $H_0$ and calculate $T(X)$ (repeat 999 times) Add $T(x)$ of sample data quantile method - calc $p$-value (proportion of $T(X)$ as extreme or more than the one of sample data $T(x)$) Pros - Can be more flexible than randomization, par or non-par; no analytic distribution of test statistic; can cope with non-iid data. Cons(compare to permutation) - need to extimate parameters for par; results vary. BootstrapBootstrapping simulations can be Parametric, simulate from a distribution that properties based on sample data (no iid assumption) Non-parametric, simulate by resampling with replacement from sample data (from Empirical DF) Percentile Method, 95% CI given by 2.5% and 97.5% percentile of the $\\mu$ set. Pros Simple, general and robust, Cons poor performance when underlying distribution is very skewed or estimator is very biased generally, only asymptotically exact as $B$ and $n \\to \\infty$ $f()$ assumption and iid assumption are respectively pro and con for Par and Non-par Bootstrapping. General Monte Carlo MethodMonte Carlo Methods are a broad class of computational algorithms that rely on repeated random sampling to generate numerical results. Used for Inference, (computer-intensive stats, e.g. MC test, Bootstrap, Bayesian stats) Optimization, (later) Stochastic Numerical Integration (compared to Deterministic one!) Optimization methods (lect 9, 10, 11)An understanding of how the methods work, ability to reproduce the algorithms and suggest which might work best in case studies, pros and cons, which to use when. An understanding of what the R functions nls, optimize and optim do. [x] Components of optimization [x] Outline of approaches: graphical, analytic, brute force, iterative: â€œbasicâ€, calculus-based, stochastic [x] Systematic: Line and grid searches [x] For iterative approaches: convergence, convergence order, stopping rules, robustification [x] Bisection; golden section [x] Newtonâ€™s method â€“ univariate, multivariate [x] Gauss-Newton [x] The EM algorithm. [x] Stochastic optimization: optimizing stochastic functions; stochastic search methods. Blind random search, localized random search, simulated annealing. [x] Not examinable: tabu methods and genetic algorithms. Why optimization? non-linear Least-squares problems Maximum Likelihood find CI via test inversion Minimizing risk in Bayesian decision analysis Components of Optimization Objective function: what we want to maximize/minimize, e.g. logLik for ML, $(y-\\hat{y})^2$ for GLS Criteria: Constraints on parameters Stopping Rule(s): desired accuracy/maximum iterations Approaches ä¼˜åŒ–é€”å¾„ Graphical - esay; but canâ€™t quantify error, hard for multivariate problems Analytic - aka differentiate func then set differential to zero; solution exact; but thereâ€™s often no analytic solution! Brute force - line/grid search (for univariate/multivariate problems); reliable, with known error; inefficient: require numerous calculation dependes on accuracy ($\\epsilon$), hard for multivariate space; â€˜Brute forceâ€™ could be used to find starting value for other methods to refine Iterative methods â€œbasicâ€: plotting, line or grid searches Calculus-based (lect 11), Newton, Gauss-Newton Stochastic (lect 12) General Iterative Algorithm Start iteration $t=0$, initial guess $x^0$ Next iteration $t=t+1$, improved guess $x^t = f(x^{t-1})$, using an updating equation Evaluate whether new guess is sufficiently accurate, using a stopping rule If yes, stop and return $x^ = x^t$ and optimized objective function $g(x^)$ If no, go back to 2 or consider whether to give up (report no convergence) Bisection SearchTake finding root of $gâ€™(x) = 0$ as an example. Start with initial interval $[a_0, b_0]$, and $gâ€™(a_0)gâ€™(b_0)&lt;0$, and systematically shrink the interval half by once. Golden Section Search More efficient (converge more quickly), as an extension of bisection method that takes andvantage of the Golden Ratio. ConvergenceEach iteration produces an answer closer to the optimum, we gradually converge on the solution we want. Convergence order is an index of how fast it converges. Convergence criterion serves as stopping rules, two main types, absolute and relative Absolute: stop when $|{x^t - x^{t-1}|} &lt; \\epsilon$, max tolerance. Problems: regardless of the size of the numbers, e.g. err of 0.1 is usually more important when $x^\\ast$ is 0.5 than it is 50,000 Relative: $\\frac{|x^t - x^{t-1}|}{|x^{t-1}|} &lt; \\epsilon$, max proportional tolerance. Robustification Use more numerically stable operation e.g. use $a+\\frac{b-a}{2}$ instead of $\\frac{a+b}{2}$ â€œgood enoughâ€ convergence (helps when $x^t$ close to zero) $$\\frac{|x^t-x^{t-1}|}{|x^{t-1}|+\\epsilon} &lt; \\epsilon$$ stop anyway after $N$ iteration, whether convergence achieved or not stop if no converging over several iterations, restart somewhere else Global optimum may not be bracketed at the beginning! or even if bracket it, there may be multiple optima, you may not turn out getting the best one. Newtonâ€™s Method (univariate)Also called â€œNewton-Raphson Methodâ€ Iterative, numerical technique for optimization, based on linear approximation with Taylor Series. Think of it as using Successive tangent lines! RationaleApproximate $gâ€™(x^\\ast)$ by linear Taylor series expansion at $x^t$$$gâ€™(x^\\ast) \\approx gâ€™(x^t) + (x^\\ast-x^t)gâ€™â€™(x^t) = 0$$ Re-arranging gives$$x^\\ast \\approx x^t - \\frac{gâ€™(x^t)}{gâ€™â€™(x^t)}$$ Here requires that $gâ€™(x)$ is continuously differentiable, and $gâ€™â€™(\\ast)\\ne 0$ So, the updaing equation is$$x^{t-1} = x^t - \\frac{gâ€™(x^t)}{gâ€™â€™(x^t)}$$ Algorithm define objective function $g(x)$ and convergence criterion tolerance $\\epsilon$ start iteration $t=0$, set $x^0$, calculate $g(x^0)$. next iteration $t=t+1$, update $x^{t+1} = x^t - gâ€™(x^t)/gâ€™â€™(x^t)$, calculate $g(x^{t+1})$. evaluate whether convergence criteria are met. if met, return $x^\\ast = x^t$ and exit; otherwise, return to step 3. Summary of Newtonâ€™s Can converge etremely rapidly - convergence order $\\beta=2$ but, may not converge at all, depending on shape of $gâ€™(x)$ Can easily get stuck in local optima Newtonâ€™s Method (multivariate) Objective function is $g(\\theta)$ where $\\theta$ is a vector of $n$ parameters $\\nabla$ and $\\nabla^2$ is the first and second derivative (gradient and Hessian) $\\Delta$ is the vector of each direction (for one step), $\\theta_{1} = \\theta_0 + \\Delta$ $$\\nabla^2 g(\\theta) \\times \\Delta = - \\nabla g(\\theta)$$ where $\\nabla^2g(\\theta)$ is an $n \\times n$ matrix; $\\Delta$ is a $1 \\times n$ vactor; â€‹ $\\nabla g(\\theta)$ is an $n$ by $n\\times 1$ vector. Algorithm Pseudocode set objective function $g(\\theta)$ and convergence tolerance $\\epsilon$ chose starting $\\theta_0$ calculate $g(\\theta_0)$ $gâ€™(\\theta) = \\nabla$ $gâ€™â€™(\\theta) = \\nabla^2$ $\\nabla^2 g(\\theta)\\Delta = - \\nabla g(\\theta)$ , solve for $\\Delta$ $\\theta_{1} = \\theta_0 + \\Delta$ Gauss-Newton Method (multivariate)Iterative procedure for extimating parameters of nonlinear least-square problems(GLS). Objective function is always the sum of square of residuals. Only first order derivatives required (advantage over Newtonâ€™s) Rationale Linearise the regression with a 1st order Taylor series evaluated at $\\theta^0$ re-arrange in the form of linear regression $$\\begin{aligned}f(x,\\theta) - f(x,\\theta^0) &amp;= \\sum_j (\\theta_j - \\theta^0_j)\\frac{df} {d\\theta_j}\\bigg|_{\\theta^0} \\&amp;\\equiv \\sum_j \\gamma_j\\omega_j\\end{aligned}$$ where $\\gamma_j = \\theta_j-\\theta_j^0$ corresponds to the unknown parameter $\\omega_j = \\frac{df}{d\\theta_j}\\Big|_{\\theta^0}$ corresponds to the known covariate Algorithm start with $i=0$, set initial parameter $\\theta^0$ estimate $\\gamma$:$$\\gamma^1 = (W^{0T}W^0)^{-1} \\bullet W^{0T}z^0$$ update $\\theta^1 = \\theta^0 + \\gamma^1$ re-evaluate $W$ and $z$ at $\\theta^1$ if met, return $\\theta$; if not go to step 2. Summary of Newton and Gauss-NewtonNewtonâ€™s Method Any objective functions require 1st and 2nd derivatives Uni and Multivariate case Gauss-Newton Method Objective function always sum of square of resid require only 1st derivatives for Multivariate non-linear least-square problem Stochastic MethodsPrevious methods are all deterministic methods. OBJ function can be evaluated exactly; algorithm is deterministic - i.e. given same data and start point, it takes same path to same solution. Why Stochastic Necessary to deal with stochastic func Can be used for discrete problem do better in finding global rather than local! some Cons; faster than brute force but slower than deterministic. Want a good solution or the best? Stochastic optimization methods use random variables. Two variaties below can overlap: Optimizing stochastic functions (OBJ function contains random noise); step size decrease, accuracy increase as iteration number increase Optimization algorithm with random search (search methods contains random element); converge faster, improve robustness when global optimum hard to find. Stochastic search Algorithm Blind random search - generate new candidate from some chosen PDF Localized random search - generate new candidate from some chosen PDF (depends on current candidate); step neither fixed(brute force) nor full random (blind) Simulated annealing - many variants, work well but sloooow Consequences of Stochasticity Evaluation of OBJ func (and its gradient) no longer accurate grid search methods may pick wrong point; iterative methods may fail to converge Accuracy can be improved by repeat eval, err decrease with $1/\\sqrt{N}$ Reproducibility and graphics (lect 12, 13, 14)Reproducible research [x] What is it and why is it a good idea [x] Ways to achieve reproducibility (or fail to achieve reproducibility) [x] Key components of reproducible research [x] Not examinable: specifics of R Markdown. Replicable, independent researchers can follow the same methods and arrive at the same conclusions. OLD. Reproducible, at least the result should be reproducible: using same data and code yield the same result Why Reproducible can reproduce the project even forget most details after long time easier to run with addtional data planning for longevity makes research more impactful Using different tools for each phase of project lead to bad reproducibility, because err can occur in any/all of these phases new data arise, whole process start over Components of Reproducibility Publication with linked and executable code and data Never touch raw data avoid mannual data manipulation version vontrol include random seeds info use relative paths Also, file project components well! Graphical presentation of data [x] What makes a good graphic; Tufteâ€™s principles; graphical integrity [x] Ability to recognize good and bad graphics and explain why they are good or bad [x] Not examinable: specifics of data visualization methods in R. Tufteâ€™s Principles numbers match true proportions dimensions match the data labels clear and detailed use well-known units to represent money not vary for some ulterior motive and not imply unintended context Lie factor - is the ratio of effect shown in graphic to effect shown in data.$$\\text{Lie Factor} = \\frac{\\text{size of effect shown in graphic}}{\\text{size of effect shown in data}}$$Chart Junk - unnecessary complexicy in graphic Code performance [x] What a profiler is and how and when to use one [x] Ability to recognize what might benefit from optimization given profiler output Profiler - software that records the instructions computer is executing When to use - after code is working as intended. Tidyverse [x] Definition of tidy data [x] Ability to recognize tidy vs. untidy data [x] Not examinable: specific R tidyverse packages and functions. Tidy data is easier to manipulate, model and visualize! define as Each variable is saved in its onw column Each observation is saved in its own row","link":"/zh-cn/MT4113-revision/"}],"tags":[{"name":"MapReduce","slug":"MapReduce","link":"/zh-cn/tags/MapReduce/"},{"name":"university","slug":"university","link":"/zh-cn/tags/university/"},{"name":"exam","slug":"exam","link":"/zh-cn/tags/exam/"},{"name":"Hadoop","slug":"Hadoop","link":"/zh-cn/tags/Hadoop/"},{"name":"Spark","slug":"Spark","link":"/zh-cn/tags/Spark/"},{"name":"Pregel","slug":"Pregel","link":"/zh-cn/tags/Pregel/"},{"name":"Mesos","slug":"Mesos","link":"/zh-cn/tags/Mesos/"},{"name":"Linux","slug":"Linux","link":"/zh-cn/tags/Linux/"},{"name":"Hexo","slug":"Hexo","link":"/zh-cn/tags/Hexo/"},{"name":"vlog","slug":"vlog","link":"/zh-cn/tags/vlog/"},{"name":"P2P","slug":"P2P","link":"/zh-cn/tags/P2P/"},{"name":"åˆ†å¸ƒå¼","slug":"åˆ†å¸ƒå¼","link":"/zh-cn/tags/åˆ†å¸ƒå¼/"},{"name":"è®ºæ–‡","slug":"è®ºæ–‡","link":"/zh-cn/tags/è®ºæ–‡/"},{"name":"è°·æ­Œ","slug":"è°·æ­Œ","link":"/zh-cn/tags/è°·æ­Œ/"},{"name":"å¤§æ•°æ®","slug":"å¤§æ•°æ®","link":"/zh-cn/tags/å¤§æ•°æ®/"},{"name":"æ—¥å¸¸","slug":"æ—¥å¸¸","link":"/zh-cn/tags/æ—¥å¸¸/"},{"name":"å‰§","slug":"å‰§","link":"/zh-cn/tags/å‰§/"},{"name":"æƒåŠ›çš„æ¸¸æˆ","slug":"æƒåŠ›çš„æ¸¸æˆ","link":"/zh-cn/tags/æƒåŠ›çš„æ¸¸æˆ/"},{"name":"å‘½ä»¤è¡Œ","slug":"å‘½ä»¤è¡Œ","link":"/zh-cn/tags/å‘½ä»¤è¡Œ/"},{"name":"æ­£åˆ™","slug":"æ­£åˆ™","link":"/zh-cn/tags/æ­£åˆ™/"},{"name":"æ•™ç¨‹","slug":"æ•™ç¨‹","link":"/zh-cn/tags/æ•™ç¨‹/"},{"name":"éŸ³é¢‘","slug":"éŸ³é¢‘","link":"/zh-cn/tags/éŸ³é¢‘/"},{"name":"å½±","slug":"å½±","link":"/zh-cn/tags/å½±/"},{"name":"é£Ÿç‰©","slug":"é£Ÿç‰©","link":"/zh-cn/tags/é£Ÿç‰©/"},{"name":"èœè°±","slug":"èœè°±","link":"/zh-cn/tags/èœè°±/"},{"name":"è§†é¢‘","slug":"è§†é¢‘","link":"/zh-cn/tags/è§†é¢‘/"},{"name":"äº‘è®¡ç®—","slug":"äº‘è®¡ç®—","link":"/zh-cn/tags/äº‘è®¡ç®—/"},{"name":"ç»Ÿè®¡","slug":"ç»Ÿè®¡","link":"/zh-cn/tags/ç»Ÿè®¡/"},{"name":"æ¨¡å‹","slug":"æ¨¡å‹","link":"/zh-cn/tags/æ¨¡å‹/"},{"name":"ç®—æ³•","slug":"ç®—æ³•","link":"/zh-cn/tags/ç®—æ³•/"}],"categories":[{"name":"Note","slug":"Note","link":"/zh-cn/categories/Note/"},{"name":"Log","slug":"Log","link":"/zh-cn/categories/Log/"}]}